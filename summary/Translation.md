# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift : 2015

## 0. Abstract
Training Deep Neural Networks is complicated by the fact
that the distribution of each layerâ€™s inputs changes during
training, as the parameters of the previous layers change.

> ì‹¬ì¸µ ì‹ ê²½ë§ì˜ í›ˆë ¨ì€ ì´ì „ layerì˜ ë§¤ê°œ ë³€ìˆ˜ê°€ ë³€ê²½ë¨ì— ë”°ë¼, í›ˆë ¨ ì¤‘ì— ê° layerì˜ ì…ë ¥ ë¶„í¬ê°€ ë³€ê²½ëœë‹¤ëŠ” ì‚¬ì‹¤ ë•Œë¬¸ì— ë³µì¡í•˜ë‹¤.

This slows down the training by requiring lower learning
rates and careful parameter initialization, and makes it `notoriously`
hard to train models with `saturating nonlinearities`.

> learning rateë¥¼ ë‚®ì¶”ê±°ë‚˜ ì´ˆê¸° íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ í•˜ë©´, í•™ìŠµì†ë„ê°€ ëŠ¦ìœ¼ì§€ë¨¸,
> ì´ ê²½ìš°ëŠ” í¬í™” ë¹„ì„ í˜•ì„±ì„ ê°€ì§„ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ì–´ë µê²Œ ë§Œë“ ë‹¤.

We refer to this phenomenon as `internal covariate
shift`, and address the problem by normalizing layer inputs.

> ìš°ë¦¬ëŠ” ì´ ìƒí™©ì„ `internal covariate shift` ë¼ê³  ë¶€ë¥´ë©°, ë ˆì´ì–´ì˜ ì…ë ¥ê°’ì„ ì •ê·œí™”
> í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

Our method draws its strength from making normalization
a part of the model architecture and performing the
normalization for each training mini-batch.

> ìš°ë¦¬ì˜ ë°©ì‹ì€ ëª¨ë¸ì˜ ê° íŒŒíŠ¸ì™€, ê°ê°ì˜ ë¯¸ë‹ˆë°°ì¹˜ë‹¨ìœ„ë“¤ì„ ì •ê·œí™”ë¥¼ ì§„í–‰í•œë‹¤.

Batch Normalization allows us to use much higher learning rates and
be less careful about initialization.

> ë°°ì¹˜ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ë†’ì€ learning rateë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, 
> ì´ˆê¸°ê°’ ìƒì„±ì— ëœ ì‹ ê²½ì„ ì“°ê²Œ í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
 
It also acts as a regularizer, in some cases eliminating the need for Dropout.

> ì´ê²ƒì€ ë˜í•œ ì •ê·œí™”ì˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨, DropuOutì˜ í•„ìš”ì„±ì„ ì¤„ì—¬ì¤€ë‹¤.

Applied to a state-of-the-art image classification model,
Batch Normalization achieves the same accuracy with 14
times fewer training steps, and `beats the original model`
by a `significant margin`. 

> SOTAë¥¼ ë‹¬ì„±í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ëª¨ë¸ì— ì ìš©í•¨ìœ¼ë¡œì¨, ê°™ì€ ì •í™•ë„ì§€ë§Œ 14ë²ˆì˜ í•™ìŠµ stepì„ ì¤„ì˜€ìœ¼ë©°,
> ê·¸ë¦¬ê³  ê¸°ì¡´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ ì•ì§ˆë €ìŠµë‹ˆë‹¤.

Using an ensemble of batch normalized networks, we improve upon the best published
result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.

> ë°°ì¹˜ì •ê·œí™” ì•™ìƒë¸”ì„ ì ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ImageNet ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.
> : top-5 ê²€ì¦ë°ì´í„°ì…‹ì˜ errorê°€ 4.9%ë¡œ ì¸ê°„ì´ í‰ê°€í•˜ëŠ” ê²ƒì„ ë›°ì–´ë„˜ì—ˆë‹¤.

---
## 1. Introduction


Deep learning has dramatically advanced the state of the art in vision, speech, and many other areas.

> ë”¥ëŸ¬ë‹ì€ ë“œë¼ë§ˆí‹±í•˜ê²Œ ë¹„ì ¼, ìŒì„± ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë°œì „ì¤‘ì´ë‹¤.

Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum (Sutskever et al., 2013) and Adagrad (Duchi et al., 2011) have been used to achieve state of the
art performance. 

> í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ì€ ê¹Šì€ ì‹ ê²½ë§ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ”ë°ì— ì¦ëª…ë˜ì—ˆìœ¼ë©°
> ê·¸ë¦¬ê³  ëª¨ë©˜í…€, AdaGrad ë“±ê³¼ ê°™ì€ SGDì˜ ë³€í˜•ì´ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.

SGD optimizes the parameters $\Theta$ of the network, so as to minimize the loss
> SGD ë„¤íŠ¸ì›Œí¬ì˜ ê²½ì‚¬($\Theta$)ë¥¼ ìµœì í™”í•˜ë©° loss ê°’ì„ ì¤„ì¸ë‹¤.


$$ \Theta = argmin_\theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$

where x1...N is the training data set. 

> x1, ... Nì€ í•™ìŠµ ë°ì´í„°ì…‹ì´ë‹¤.

With SGD, the training proceeds in steps, and at each step we consider a minibatch x1...m of size m. 

> SGDë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, í•™ìŠµì€ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰ë˜ë©° ìš°ë¦¬ëŠ” ë¯¸ë‹ˆë°°ì¹˜ x1 ë¶€í„° mê¹Œì§€ì˜ ë‹¨ê³„ë¥¼ ê³ ë ¤í•˜ë‚Ÿ.

The mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing

> ë¯¸ë‹ˆë°°ì¹˜ëŠ” ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì ì˜ ê²½ì‚¬ì— ê·¼ì ‘í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. ì•„ë˜ì™€ ê°™ì€ ì—°ì‚°ì„ í†µí•´ì„œ..

$$\frac{1}{m}\frac{\partial \ell(x_i, \Theta)}{\partial \Theta}$$


Using mini-batches of examples, as `opposed` to one example
at a time, is helpful in several ways.

> ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ ë°©ë©´ì— ë„ì›€ì„ ì¤€ë‹¤.

First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. 

> ì²«ì§¸, ë¯¸ë‹ˆ ë°°ì¹˜ì— ëŒ€í•œ ì†ì‹¤ì˜ ê¸°ìš¸ê¸°ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ í’ˆì§ˆì´ í–¥ìƒë˜ëŠ” í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•œ ê¸°ìš¸ê¸°ì˜ ì¶”ì •ì¹˜ì´ë‹¤.

Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.

> ë‘˜ì§¸, ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ìµœê·¼ ê³„ì‚° í”Œë«í¼ë“¤ì´ ë³‘ë ¬ì²˜ë¦¬ë¥¼ í—ˆìš©í•˜ê¸° ë•Œë¬¸ì—, ê°ê°ì„ më²ˆ ê³„ì‚°í•˜ëŠ”ê²ƒë³´ë‹¤ ë” íš¨ìœ¨ì ì´ë‹¤.

While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. 

> ë°˜ë©´, SGDëŠ” ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì´ì§€ë§Œ, ê·¸ê²ƒì€ hyper parameter íŠœë‹ì„ ì •êµí•˜ê²Œ í•´ì•¼í•˜ë©°, í•™ìŠµìœ¨ë˜í•œ ì •ê·œí•˜ê²Œ ì‘ì—…í•´ì•¼í•œë‹¤. ë˜í•œ ì´ˆê¸°ê°€ì¤‘ì¹˜ë„ ê·¸ë ‡ë‹¤.

The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers â€“ so that small changes to the network parameters amplify as the network becomes deeper.

> ê·¸ í•™ìŠµì€ ëª¨ë“  inputì´ ê°ê°ì˜ layerê°€ ì§„í–‰ë ëŒ€ë§ˆë‹¤ ì˜í–¥ì„ ë¼ì¹˜ê¸° ë•Œë¬¸ì—, ê¹Šì–´ì§ˆìˆ˜ë¡ ì˜í–¥ì´ í¬ë‹¤.

The change in the distributions of layersâ€™ inputs presents a problem because the layers need to continuously adapt to the new distribution. 

> ê° ì¸µì˜ ë ˆì´ì–´ë“¤ì€ ìƒˆë¡œìš´ ë¶„í¬ì— ëŒ€í•œ ë³€í™”ë¥¼ í•„ìš”ë¡œ í•œë‹¤.

When the input distribution to a learning system changes, it is said to experience `covariate shift` (Shimodaira, 2000). 

> í•™ìŠµ ë•Œ, ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë³€í•œë‹¤ë©´ covariate shift(ê³µë³€ëŸ‰ ì´ë™)ë¥¼ ê²ªëŠ”ë‹¤.

This is typically handled via `domain adaptation`(Jiang, 2008). 

> ì´ê²ƒì€ ë„ë©”ì¸ ì ì‘(?)ì„ í†µí•´ í•´ê²°ëœë‹¤.

However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. 

> ê·¸ëŸ¬ë‚˜ ê³µë³€ëŸ‰ ì´ë™ì˜ ê°œë…ì€ í•™ìŠµ ì‹œìŠ¤í…œ ì „ì²´ ë¿ ì•„ë‹ˆë¼, í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ ë° ë ˆì´ì–´ì—ë„ ì˜í–¥ì„ ë¼ì¹œë‹¤.

Consider a network computing
> ì•„ë˜ì™€ ê°™ì€ network ì—°ì‚°ì„ ê³ ë ¤í•œë‹¤.

$$\ell = F_2(F_1(u, \Theta_1), \Theta_2)$$

where F1 and F2 are arbitrary transformations, and the parameters â€€$\Theta_1, \Theta_2$ are to be learned so as to minimize the loss â„“. 

> F1, F2ëŠ” ì„ì˜ì˜ ë³€í˜•ê°’ì´ë©°, íŒŒë¼ë¯¸í„° $\Theta_1, \Theta_2$ ëŠ” loss $\ell$ ì„ ìµœì†Œí™”ì‹œí‚¤ë„ë¡ í•™ìŠµí•œë‹¤.

Learning $\Theta_2$ can be viewed as if the inputs $x = F_1(u,â€€\Theta_1)$ are fed into the sub-network
> $\Theta_2$ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ $x = F_1(u,â€€\Theta_1)$ë¥¼ inputìœ¼ë¡œ í•˜ì—¬, í•˜ìœ„ ê³„ì¸µ ë„¤íŠ¸ì›Œí¬ì— ì…ë ¥ë˜ëŠ”ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

$$\ell = F_2(x, \Theta_2)$$

For example, a gradient descent step
> ê²½ì‚¬ í•˜ê°•ë²• ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \Theta_2 \leftarrow \Theta_2 - \frac{\alpha}{m} \sum_i^m \frac{\partial F_2(x_i), \Theta_2}{\partial \Theta_2} $$


(for batch size $m$ and learning rate Î±) is exactly equivalent to that for a `stand-alone network` $F_2$ with input $x$. 

> ë°°ì¹˜ì‚¬ì´ì¦ˆ $m$ê³¼ í•™ìŠµë¥  $\alpha$ëŠ” ì •í™•íˆ stand-alone ë„¤íŠ¸ì›Œí¬ì¸ $x$ë¥¼ inputìœ¼ë¡œ í•˜ëŠ” $F_2$ì™€ ì •í™•íˆ ë™ì¼í•˜ë‹¤.

Therefore, the input distribution properties that make training more efficient â€“ such as having the same distribution between the training and test data â€“ apply to training the sub-network as well. 

> ë”°ë¼ì„œ input ë°ì´í„°ì˜ ë¶„í¬ íŠ¹ì„±ì€ í•™ìŠµì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“ ë‹¤. such as)í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ì˜ ë¶„í¬ê°€ ê°™ì„ ê²½ìš° - í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ì—ë„ ì˜ ì ìš©ëœë‹¤.

As such it is advantageous for the distribution of $x$ to remain fixed over time. 

> ë”°ë¼ì„œ, $x$ì˜ ë¶„í¬ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë”ë¼ë„ ìœ ì§€ë˜ëŠ”ê²ƒì´ ìœ ë¦¬í•˜ë‹¤. 

Then, $\Theta_2$ does not have to readjust to compensate for the change in the distribution of x.
> ê·¸ë ‡ê²Œëœë‹¤ë©´, $\Theta_2$ëŠ” $x$ì˜ ë¶„í¬ ì¡°ì •ì„ ìœ„í•´ ì¬ì¡°ì • í•  í•„ìš”ê°€ ì—†ë‹¤.

Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the subnetwork, as well.

> ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•œ ì…ë ¥ì˜ ê³ ì •ëœ ë¶„í¬ëŠ” í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ ë°–ì˜ ê³„ì¸µë“¤ì—ê²Œë„ ê¸ì •ì ì¸ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì´ë‹¤.

Consider a layer with a sigmoid activation function $z = g(Wu + b)$ where $u$ is the layer input, the weight matrix $W$ and bias vector $b$ are the layer parameters to be learned, and $g(x) = \frac{1}{1+exp(âˆ’x)}$. As $|x|$ increases, $gâ€²(x)$ tends to zero.

> ìœ„ì™€ ê°™ì€ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”í•¨ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬, $u$ê°€ ë ˆì´ì–´ì˜ inputì„ì„ ê³ ë ¤í•˜ë©´ , $W$ì™€ $b$ëŠ” ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ì—ì˜í•´ í•™ìŠµë ê²ƒì´ë©°, $|x|$ ê°€ ë†’ì„ìˆ˜ë¡ ê²½ì‚¬ëŠ” 0ì— ê·¼ì‚¬í•˜ê²Œ ë  ê²ƒì´ë‹¤.


This means that for all dimensions of $x = Wu+b$ except those with small absolute values, the gradient flowing down to $u$ will vanish and the model will train slowly.

> ì´ê²ƒì€ ëª¨ë“  ì°¨ì›ì´ $x$ì˜ ì ˆëŒ€ê°’ì„ ì‚¬ìš©í•˜ë©´, ì ˆëŒ€ê°’ì´ ì‘ì€ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” $u$ëŠ” ì†Œë©¸ë  ê²ƒì´ë‹¤.

However, since $x$ is affected by $W, b$ and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of $x$ into the `saturated regime` of the nonlinearity and slow down the convergence

> ê·¸ëŸ¬ë‚˜ xëŠ” W, bì™€ ì•„ë˜ì˜ ëª¨ë“  ì¸µì˜ íŒŒë¼ë¯¸í„°ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ì— xì˜ ë§ì€ ì°¨ì›ì„ ë¹„ì„ í˜•ì„±ì˜ í¬í™”ìƒíƒœë¡œ ì´ë™ì‹œí‚¤ê³  ìˆ˜ë ´ì„ ëŠ¦ì¶œ ê²ƒì´ë‹¤.


This effect is amplified as the network depth increases. 

> ì´ëŸ¬í•œ íš¨ê³¼ëŠ” ë„¤íŠ¸ì›Œí¬ ê¹Šì´ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì¦í­ë©ë‹ˆë‹¤.

In practice, the `saturation problem` and the resulting vanishing gradients are usually addressed by using `Rectified Linear Units` (Nair & Hinton, 2010) ReLU(x) = max(x, 0), careful initialization (Bengio & Glorot, 2010; Saxe et al., 2013), and small learning rates. 

> ê²½ì‚¬ ì†Œë©¸ ë° ê¸‰ê²©í•˜ê²Œ ê²½ì‚¬ê°€ ë³€í™”ëŠ” ë¬¸ì œëŠ” ReLU í™œì„±í™”í•¨ìˆ˜ë¥¼ í†µí•´ ì–´ëŠì •ë„ í•´ê²°ì´ ë˜ì—ˆì—ˆë‹¤. ë˜í•œ ì´ˆê¸°ê°’ì„ ë‹¤ë£¨ê±°ë‚˜ learning rateë¥¼ ë°”ê¾¸ëŠ” ë°©ì‹ìœ¼ë¡œë„.

If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate.

> ê·¸ëŸ¬ë‚˜ ë¹„ì„ í˜•ì„±ì¸ Input ì´ ì¢€ë” ì•ˆì •ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì— ë“¤ì–´ì™€ì„œ í•™ìŠµì´ ëœë‹¤ë©´, í¬í™”ìƒíƒœì— ë¹ ì§ˆ ê°€ëŠ¥ì„±ì´ ë‚®ì•„ì§€ê³ , í›ˆë ¨ì†ë„ ë˜í•œ ê°€ì†í™”ë  ê²ƒì´ë‹¤.

We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as `Internal Covariate Shift`. 

> ìš°ë¦¬ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ ë…¸ë“œ ë¶„í¬ì˜ ë³€í™”ë¥¼ ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì´ë¼ê³  í•œë‹¤.

Eliminating it offers a promise of faster training. 

> ì´ê²ƒì„ ì œê±°í•¨ìœ¼ë¡œì¨ í•™ìŠµì†ë„ê°€ ë¹¨ë¼ì§„ë‹¤.

<font color='red'>We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets.  </font>

> ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ ì¤„ì´ê³ , ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ì‹¬ì¸µ ì‹ ê²½ë§ì˜ í›ˆë ¨ì„ íšê¸°ì ìœ¼ë¡œ ê°€ì†í™”í•˜ëŠ” ë°°ì¹˜ ì •ê·œí™”ë¼ê³  ë¶ˆë¦¬ëŠ” ìƒˆë¡œìš´ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•œë‹¤.


It accomplishes this via a normalization step that fixes the means and variances of layer inputs. 

> ì´ê²ƒì€ ê³„ì¸µ ì…ë ¥ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ìˆ˜ì •í•˜ëŠ” ì •ê·œí™” ë‹¨ê³„ë¥¼ í†µí•´ ë‹¬ì„±ëœë‹¤.

Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values.

> ë°°ì¹˜ ì •ê·œí™”ëŠ” ë˜í•œ ë§¤ê°œ ë³€ìˆ˜ì˜ ì²™ë„ ë˜ëŠ” ì´ˆê¸° ê°’ì— ëŒ€í•œ ê·¸ë ˆì´ë””ì–¸íŠ¸ì˜ ì˜ì¡´ì„±ì„ ì¤„ì„ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ê·¸ë ˆì´ë””ì–¸íŠ¸ íë¦„ì— ìœ ìµí•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

This allows us to use much higher learning rates without the risk of divergence. 

> ì´ê²ƒì€ í•™ìŠµìœ¨ì„ ë†’ì—¬ë„, ê²½ì‚¬ê°€ ë°œì‚°í•˜ëŠ” ë¬¸ì œê°€ ì—†ë„ë¡ í•œë‹¤.

Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014).

> ê²Œë‹¤ê°€, ë°°ì¹˜ì •ê·œí™”ëŠ” ëª¨ë¸ì— ê·œì œë¥¼ í•´ì£¼ë©°, DropOutì˜ í•„ìš”ì„±ì„ ì¤„ì—¬ì¤€ë‹¤.

Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.

> ë§ˆì§€ë§‰ìœ¼ë¡œ, ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ë¹„ì„ í˜•ì ì¸ í™œì„±í™”í•¨ìˆ˜ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤??


In Sec. 4.2, we apply Batch Normalization to the bestperforming ImageNet classification network, and show that we can match its performance using only 7% of the training steps, and can further exceed its accuracy by a `substantial margin`

> 4.2ì ˆì—ì„œëŠ” Batch Normalizationì„ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ImageNet ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ì— ì ìš©í•˜ê³  í•™ìŠµ ë‹¨ê³„ì˜ 7%ë§Œ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìœ¼ë©° ì •í™•ë„ë„ í›¨ì”¬ ëŠ¥ê°€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

Using an ensemble of such networks trained with Batch Normalization, we achieve the top-5 error rate that improves upon the best known results on ImageNet classification.

> ë°°ì¹˜ì •ê·œí™” ì•™ìƒë¸”ì„ ì ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ImageNet ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.
> : top-5 ê²€ì¦ë°ì´í„°ì…‹ì˜ errorê°€ 4.9%ë¡œ ì¸ê°„ì´ í‰ê°€í•˜ëŠ” ê²ƒì„ ë›°ì–´ë„˜ì—ˆë‹¤.

## 2. Towards Reducing Internal Covariate Shift

We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. 

> ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ í›ˆë ¨ ì¤‘ ë„¤íŠ¸ì›Œí¬ ë§¤ê°œ ë³€ìˆ˜ì˜ ë³€í™”ë¡œ ì¸í•œ ë„¤íŠ¸ì›Œí¬ í™œì„±í™” ë¶„í¬ì˜ ë³€í™”ë¡œ ì •ì˜í•œë‹¤.

To improve the training, we seek to reduce the internal covariate shift.

> í›ˆë ¨ì„ ê°œì„ í•˜ê¸° ìœ„í•˜ì—¬,  ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ ì¤„ì´ëŠ” ë°©ë²•ì„ íƒìƒ‰í•œë‹¤.

By fixing the distribution of the layer inputs x as the training progresses, we expect to improve the training speed.

> í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ë ˆì´ì–´ ì…ë ¥ xì˜ ë¶„í¬ë¥¼ ê³ ì •í•¨ìœ¼ë¡œì¨ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are `whitened` â€“ i.e., linearly transformed to have zero means and unit variances, and `decorrelated`. 

> ì…ë ¥ë°ì´í„°ê°€ whitend(? : ì•„ë§ˆ í‘œì¤€í™”ê°™ì€ê°œë…) ìˆ˜ë ´ì†ë„ê°€ ë” ë¹¨ë¼ì§„ë‹¤ëŠ” ê²ƒì€ ì´ë¯¸ ì˜ ì•Œë ¤ì ¸ìˆë‹¤. - í‰ê·  0ì„ ê°–ìœ¼ë©°, ë‹¨ìœ„ ë¶„ì‚°ê³¼ ë¹„ìƒê´€ì„±ì„ ê°–ëŠ”ë‹¤. 

As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer.

> ê° ì¸µì´ ì•„ë˜ ì¸µì— ì˜í•´ ìƒì„±ëœ ì…ë ¥ì„ ê´€ì°°í•˜ê¸° ë•Œë¬¸ì— ê° ì¸µì˜ ì…ë ¥ì— ëŒ€í•´ ë™ì¼í•œ í™”ì´íŠ¸ë‹ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•  ê²ƒì´ë‹¤.

By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.

> ê° ì¸µì— ëŒ€í•œ ì…ë ¥ì„ whitening í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì˜ ë¶€ì‘ìš©ì„ ì œê±°í•˜ê¸° ìœ„í•´,  ì…ë ¥ì˜ ê³ ì •ëœ ë¶„í¬ë¥¼ ìœ„í•´ ë‹¨ê³„ë¥¼ ì§„í–‰í• ê²ƒì´ë‹¤.


We could consider whitening activations at every training step or at some interval, either by __modifying the
network directly__ or by __changing the parameters of the optimization algorithm__ to depend on the `network activation values` (Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins & Kavukcuoglu).

> ìš°ë¦¬ëŠ” networkë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ê±°ë‚˜, íŒŒë¼ë¯¸í„° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ë³€ê²½í•¨ìœ¼ë¡œì¨ ê° ë‹¨ê³„ë³„ whitening í™œì„±í™”ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤. 
> ì´ëŠ” network activation valueì— ì˜ì¡´ì ì´ë‹¤. (í™œì„±í™”í•¨ìˆ˜ ë§í•˜ëŠ”ê±´ì§€?)

However, if these modifications are `interspersed` with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect
of the gradient step

> ê·¸ëŸ¬ë‚˜ ì´ ê²½ìš° ë“œë¬¸ë“œë¬¸ ìµœì í™” ë‹¨ê³„ê°€ ìˆë‹¤ë©´, ê²½ì‚¬ í•˜ê°• ë‹¨ê³„ëŠ” ì •ê·œí™”ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë ¤ê³  ì‹œë„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê²½ì‚¬ ë‹¨ê³„ì˜ ì˜í–¥ì„ ê°ì†Œì‹œí‚¨ë‹¤.

For example, consider a layer with the __input $u$__ that adds the learned bias $b$, and normalizes the result by subtracting the mean of the activation computed over the training data: $\hat{x} = x âˆ’ E[x]$ where $x = u + b$, $X = \{x1...N\}$ is the set of values of $x$ over the training set, and $E[x] = \frac{1}{N}\sum_i^N xi$.

> ë‹¨ê³„ë³„ë¡œ input dataì— ëŒ€í•´ì„œ ì •ê·œí™” ê³¼ì •ì„ ì§„í–‰í•˜ëŠ”ê²ƒìœ¼ë¡œ í•´ì„ë¨.

If a gradient descent step ignores the dependence of $E[x]$ on $b$, then it will update $b \leftarrow b + \triangle b$, where â€€$\triangle b \propto âˆ’âˆ‚â„“/âˆ‚ \hat{x}$
> ë§Œì•½ ê²½ì‚¬í•˜ê°•ë²•ì´ ê¸°ëŒ€ê°’$E[x]$ ì™€ $bias$ë¥¼ ë¬´ê¸°í•œë‹¤ë©´ ìœ„ì™€ ê°™ì´ ì—…ë°ì´íŠ¸ ë  ê²ƒì´ë‹¤.

Then $u+(b+\triangle b)-E[u+(b+\triangle b)]=u+b-E[u+b]$.
> ìœ„ì™€ ê°™ì´ ì—…ë°ì´íŠ¸ê°€ ëì„ ê²ƒì´ë‹¤?

Thus, the combination of the update to b and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. 

> ê·¸ëŸ¬ë¯€ë¡œ, $b$ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ì™€ ê·¸ì— ë”°ë¥¸ ì •ê·œí™” ë³€ê²½ì˜ ì¡°í•©ì€ ê³„ì¸µ ì¶œë ¥ì˜ ë³€í™”ë„, ê²°ê³¼ì ìœ¼ë¡œ ì†ì‹¤ë„ ì´ˆë˜í•˜ì§€ ì•Šì•˜ë‹¤.

As the training continues, $b$ will grow indefinitely while the loss remains fixed.

> í›ˆë ¨ì´ ê³„ì†ë ìˆ˜ë¡ $b$ëŠ” ì†ì‹¤ì´ ê³ ì •ëœ ìƒíƒœì—ì„œ ë¬´í•œ ì„±ì¥í•œë‹¤.

This problemcan get worse if the normalization not only centers but also scales the activations.
> ì´ ë¬¸ì œëŠ” ì •ê·œí™”ê°€ ì¤‘ì‹¬ì¼ ë¿ë§Œ ì•„ë‹ˆë¼ í™œì„±í™” í•¨ìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§ í•œë‹¤ë©´, ë” ì•…í™”ë  ìˆ˜ ìˆë‹¤.

We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step.

> ì •ê·œí™” ë§¤ê°œ ë³€ìˆ˜ê°€ ê²½ì‚¬ í•˜ê°• ë‹¨ê³„ ë°–ì—ì„œ ê³„ì‚°ë  ë•Œ ëª¨ë¸ì´ í­íŒŒë˜ëŠ” ì´ˆê¸° ì‹¤í—˜ì—ì„œ ì´ë¥¼ ê²½í—˜ì ìœ¼ë¡œ ê´€ì°°í–ˆë‹¤.

The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. 

> ã…‹ã…‹

<font color='red'>To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution.  </font>

> ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ ê°’ì— ëŒ€í•´ ë„¤íŠ¸ì›Œí¬ê°€ í•­ìƒ ì›í•˜ëŠ” ë¶„í¬ë¡œ í™œì„±í™”ë¥¼ ìƒì„±í•˜ë„ë¡ ë³´ì¥í•˜ê³ ì í•œë‹¤.


Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters $\Theta$.

> ê·¸ë ‡ê²Œ í•˜ëŠ” ê²ƒì€ ì •ê·œí™”ì™€ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ $\Theta$ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤ì˜ ê¸°ìš¸ê¸°ë¥¼ í—ˆìš©í•  ê²ƒì´ë‹¤.

Let again x be a layer input, treated as a vector, and X be the set of these inputs over the training data set. 

> ë‹¤ì‹œ xë¥¼ ë²¡í„°ë¡œ ì·¨ê¸‰í•˜ëŠ” ë ˆì´ì–´ ì…ë ¥ìœ¼ë¡œ í•˜ê³  Xë¥¼ í›ˆë ¨ ë°ì´í„° ì§‘í•©ì— ëŒ€í•œ ì´ëŸ¬í•œ ì…ë ¥ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ í•˜ì.

The normalization can then be written as a transformation
> ê·¸ ì •ê·œí™”ì˜ ë³€í™˜ì€ ì•„ë˜ì‹ì²˜ëŸ¼ ì“¸ ìˆ˜ ìˆë‹¤.

$$\hat{x} = Norm(x, \chi)$$

which depends not only on the given training example $x$ but on all examples $\chi$ â€“ each of which depends on $\Theta$ if $x$ is generated by another layer. For backpropagation, we would need to compute the `Jacobians`

> ì´ëŠ” ì£¼ì–´ì§„ í•™ìŠµ ì˜ˆì œ ğ‘¥ ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  ì˜ˆì œ ğœ’ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ”ë°, ê° ì˜ˆì œ ğœ’ëŠ” $x$ê°€ ë‹¤ë¥¸ ê³„ì¸µì— ì˜í•´ ìƒì„±ë  ê²½ìš° Î˜ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. 
> ì—­ì „íŒŒë¥¼ ìœ„í•´ì„œëŠ” ìì½”ë¹„ì•ˆë“¤ì„ ê³„ì‚°í•œë‹¤.

$$ \frac{\partial Norm(x, \chi)}{\partial x} and \frac{\partial Norm(x, \chi)}{\partial \chi} ;$$

ignoring the latter term would lead to the explosion described above. 
> í›„ìì˜ ìš©ì–´ë¥¼ ë¬´ì‹œí•˜ë©´, ìœ„ì—ì„œ ì„¤ëª…í•œ ê¸°ìš¸ê¸° í­ë°œì´ ì¼ì–´ë‚  ê²ƒì´ë‹¤.

Within this framework, whitening the layer inputs is expensive, as it requires computing the `covariance matrix` $Cov[x] = Ex_{âˆˆ\chi} [xxT ] âˆ’ E[x]E[x]^T$ and its inverse square root, to produce the `whitened activations` $Cov[x]^{-1/2}(x âˆ’ E[x])$, as well as the derivatives of these transforms for backpropagation. 

> ì´ í”„ë ˆì„ì›Œí¬ì—ì„œëŠ”, layer inputì„ whkiteningí•˜ëŠ” ì—°ì‚°ì€ ë¹„ì‹¸ë‹¤. ìš°ë¦¬ëŠ” ì—°ì‚°ì„ ìœ„í•´ ê³µë¶„ì‚°í–‰ë ¬ì„ í•„ìš”ë¡œ í•˜ë©°, ê·¸ê²ƒì€ ì—­ìˆ˜ ë° ë£¨íŠ¸ë¥¼ ì”Œì›Œì„œ `whitend activations`ë¥¼ ë§Œë“ ë‹¤. ìš°ë¦¬ëŠ” ì—­ì „íŒŒë¥¼ ìœ„í•´ ì´ê²ƒì„ íŒŒìƒí•œë‹¤.

This motivates us to seek an alternative that performs input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update.

> ì´ê²ƒì€ ìš°ë¦¬ê°€ ì°¨ë³„í™” ê°€ëŠ¥í•˜ê³  ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ í›„ ì „ì²´ í•™ìŠµë°ì´í„°ì˜ ë¶„ì„ì„ ìš”êµ¬í•˜ì§€ ì•ŠëŠ” ë°©ì‹ìœ¼ë¡œ ì…ë ¥ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì•ˆì„ ì°¾ë„ë¡ ë§Œë“ ë‹¤.

Some of the previous approaches (e.g. (Lyu & Simoncelli, 2008)) use statistics computed over a single training example, or, in the case of image networks, over different feature maps at a given location.

> ì´ì „ ì ‘ê·¼ë²• ì¤‘ ì¼ë¶€ëŠ” ë‹¨ì¼ í›ˆë ¨ ì˜ˆì‹œì— ëŒ€í•´, ë˜ëŠ” ì´ë¯¸ì§€ ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìš° ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ ë‹¤ë¥¸ íŠ¹ì§•ë§µì— ëŒ€í•´ ê³„ì‚°ëœ í†µê³„ê°’ ì‚¬ìš©í•œë‹¤.

However, this changes the representation ability of a network by discarding the absolute scale of activations.
> ê·¸ëŸ¬ë‚˜ ì´ëŠ” í™œì„±í™”ì˜ ì ˆëŒ€ì ì¸ scaleì„ ë²„ë¦¼ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ì •ë³´ë¥¼ ë³€í™”ì‹œí‚¨ë‹¤.

We want to a preserve the information in the network, by normalizing the activations in a training example relative to the statistics of the entire training data.
> ìš°ë¦¬ëŠ” ì „ì²´ í›ˆë ¨ ë°ì´í„°ì˜ í†µê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í›ˆë ¨ ì˜ˆì œì˜ í™œì„±í™”ë¥¼ ì •ê·œí™”í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ì˜ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³ ì í•œë‹¤.

##  3. Normalization via Mini-Batch Statistics

Since the full whitening of each layerâ€™s inputs is costly and not everywhere differentiable, we make two necessary simplifications. 

> ê° ì¸µì˜ ì…ë ¥ì— ëŒ€í•œ ì™„ì „í•œ í™”ì´íŠ¸ë‹ì€ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì–´ë””ì—ì„œë‚˜ êµ¬ë³„í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ í•„ìš”í•œ ë‹¨ìˆœí™”ë¥¼ í•œë‹¤.

The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have the mean of zero and the variance of 1. 

> ì²« ë²ˆì§¸ëŠ” ë ˆì´ì–´ ì…ë ¥ê³¼ ì¶œë ¥ì˜ íŠ¹ì§•ì„ ê³µë™ìœ¼ë¡œ í™”ì´íŠ¸ë‹í•˜ëŠ” ëŒ€ì‹ , ìš°ë¦¬ëŠ” ê° ìŠ¤ì¹¼ë¼ íŠ¹ì§•ì„ 0ê³¼ 1ì˜ ë¶„ì‚°ìœ¼ë¡œ ë§Œë“¤ì–´ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”í•  ê²ƒì´ë‹¤.

For a layer with $d$-dimensional input $x = (x^{(1)} . . . x^{(d)})$, we will normalize each dimension

> ìœ„ì™€ê°™ì€ $d$ ì°¨ì›ì˜ inputì´ ë“¤ì–´ì˜¨ë‹¤ë©´, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ê°™ì´ ê°ê°ì˜ ì°¨ì›ì€ ì •ê·œí™”ì‹œí‚¬ê²ƒì´ë‹¤.

$$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}] }{\sqrt{Var[x^{(k)}]}}$$


where the expectation and variance are computed over the training data set. 

> ì—¬ê¸°ì„œ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

As shown in (LeCun et al., 1998b), such normalization speeds up convergence, even when the features are not decorrelated.

> (LeCun et al., 1998b)ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì´ëŸ¬í•œ ì •ê·œí™”ëŠ” íŠ¹ì§•ì´ ìƒê´€ê´€ê³„ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ìˆ˜ë ´ ì†ë„ë¥¼ ë†’ì¸ë‹¤.

Note that simply normalizing each input of a layer may change what the layer can represent. 

> ë‹¨ìˆœíˆ ê³„ì¸µì˜ ê° ì…ë ¥ì„ ì •ê·œí™”í•˜ëŠ” ê²ƒì€ ê³„ì¸µì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ê²ƒì„ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì£¼ëª©í•˜ë¼.

For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. 

> ì˜ˆë¥¼ ë“¤ì–´, ì‹œê·¸ëª¨ì´ë“œì˜ ì…ë ¥ì„ ì •ê·œí™”í•˜ë©´ ë¹„ì„ í˜•ì„±ì˜ ì„ í˜•ìœ¼ë¡œ ì œí•œëœë‹¤.

To address this, we make sure that the transformation inserted in the network can represent the `identity transform`.
> ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë„¤íŠ¸ì›Œí¬ì— ì‚½ì…ëœ ë³€í™˜ì´ identity transform(? : ê¸°ë³¸ ì •ë³´ë¡œ ì¶”ì¸¡) ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.

To accomplish this, we introduce, for each activation $x^{(k)}$, a pair of parameters $Î³^{(k)}, Î²^{(k)}$, which scale and shift the normalized value:

> ê°ê°ì˜ ëª¨ë“  í™œì„±í™”í•¨ìˆ˜ $x^{(k)}$ì— ëŒ€í•´, ê°ê°ì˜ ìŒì¸ $Î³^{(k)}, Î²^{(k)}$,ëŠ” ìŠ¤ì¼€ì¼ë§ ë° ì •ê·œí™”ê°’ì„ shiftí•œë‹¤.

These parameters are learned along with the original model parameters, and restore the representation power
of the network. 

> ì´ëŸ¬í•œ ë§¤ê°œ ë³€ìˆ˜ëŠ” ì›ë˜ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ì™€ í•¨ê»˜ í•™ìŠµë˜ê³  ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ ëŠ¥ë ¥ì„ ë³µì›í•œë‹¤.

Indeed, by setting $Î³^{(k)} = \sqrt{Var[x(k)]}$ and $Î²^{(k)} = E[x^{(k)}]$, we could recover the original activations,
if that were the optimal thing to do.

> $Î³^{(k)} = \sqrt{Var[x(k)]}$ and $Î²^{(k)} = E[x^{(k)}]$ì™€ ê°™ì´ ì„¤ì •í•œë‹¤ë©´, ìš°ë¦¬ëŠ” ê¸°ë³¸ í™œì„±í™”ë¥¼ ë³µì›í•  ìˆ˜ ìˆë‹¤.


In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations.

> ê° í›ˆë ¨ ë‹¨ê³„ê°€ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë°°ì¹˜ ì„¤ì •ì—ì„œ ì „ì²´ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í™œì„±í™”ë¥¼ ì •ê·œí™”í•œë‹¤.

However, this is impractical when using stochastic optimization. 

> ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ í™•ë¥ ì  ìµœì í™”ë¥¼ ì‚¬ìš©í•  ë•Œ ë¹„í˜„ì‹¤ì ì´ë‹¤.

Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation.

> ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‘ ë²ˆì§¸ ë‹¨ìˆœí™”ë¥¼ í•œë‹¤ : í™•ë¥ ì  ê²½ì‚¬ í›ˆë ¨ì—ì„œ ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê° ë¯¸ë‹ˆ ë°°ì¹˜ëŠ” ê° í™œì„±í™”ì˜ í‰ê· ê³¼ ë¶„ì‚°ì— ëŒ€í•œ ì¶”ì •ì¹˜ë¥¼ ìƒì„±í•œë‹¤.

This way, the statistics used for normalization can fully participate in the gradient backpropagation. 

> ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”ì— ì‚¬ìš©ë˜ëŠ” í†µê³„ëŠ” ê·¸ë ˆì´ë””ì–¸íŠ¸ ì—­ì „íŒŒì— ì™„ì „íˆ ì°¸ì—¬í•  ìˆ˜ ìˆë‹¤.

Note that the use of minibatches is enabled by __computation of per-dimension variances__ rather than joint covariances; 

> ë¯¸ë‹ˆë°°ì¹˜ì˜ ì‚¬ìš©ì€ ì „ì²´ ê³µë¶„ì‚°ë³´ë‹¤ëŠ” ì°¨ì›ë‹¹ ë¶„ì‚° ê³„ì‚°ì— ì˜í•´ ê°€ëŠ¥í•˜ë‹¤.

in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices.

> ì´ ì „ì²´ ê²½ìš°, ë¯¸ë‹ˆ ì§„ë™ í¬ê¸°ê°€ í°ìƒ‰ìœ¼ë¡œ ë³€í•˜ëŠ” í™œì„±ì˜ ìˆ˜ë³´ë‹¤ ì‘ì„ ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì— ì •ê·œí™”ê°€ ìš”êµ¬ë˜ì–´ ë‹¨ìˆ˜ ê³µë¶„ì‚° í–‰ë ¬ì´ ìƒì„±ëœë‹¤.

Consider a mini-batch B of size m.

> í¬ê¸°ê°€ mì¸ ë¯¸ë‹ˆë°°ì¹˜ Bë¥¼ ê³ ë ¤í•´ë³´ì. 

Since the normalization is applied to each activation independently, let us focus on a particular activation $x^{(k)}$ and omit $k$ for clarity. 

>ì •ê·œí™”ëŠ” ê° í™œì„±í™”ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ë¯€ë¡œ íŠ¹ì • í™œì„±í™” $x^{(k)}$ì— ì´ˆì ì„ ë§ì¶”ê³  ëª…í™•ì„±ì„ ìœ„í•´ $k$ë¥¼ ìƒëµí•©ì‹œë‹¤.

We have $m$ values of this activation in the mini-batch,

>ìš°ë¦¬ëŠ” ë¯¸ë‹ˆë°°ì¹˜ì•ˆì—ì„œ $m$ ê°’ì˜ í™œì„±í™”ë¥¼ ê°–ëŠ”ë‹¤.

$$ B = \{{x_{1...m}}\}.$$

Let the normalized values be $\hat{x}_{1...m}$, and their linear transformations be $y_{1...m}$. 

> $\hat{x}_{1...m}$ ê°’ì„ ì •ê·œí™” í•˜ê³ , $y_{1...m}$ ë¡œ ì„ í˜•ë³€í™˜ í•˜ì.

We refer to the transform as the <font color='red'>Batch Normalizing Transform</font>. 

> ì•„ë˜ì™€ ê°™ì€ Batch Normalizing Transform ì„ ë³¼ ìˆ˜ ìˆë‹¤.

$$BN_{\gamma,\beta} : x_{1...m} \rightarrow y_{1...m}$$

We present the BN Transform in Algorithm1. 

> ìš°ë¦¬ëŠ” Algoritm1ì— ë³€í™˜ê³¼ì •ì„ í‘œí˜„í•œë‹¤.

In the algorithm, $\epsilon$ is a constant added to the mini-batch variance for `numerical stability`.

> í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì—ì„œ $\epsilon$ëŠ” ìˆ˜ì¹˜ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë¯¸ë‹ˆ ë°°ì¹˜ ë¶„ì‚°ì— ì¶”ê°€ë˜ëŠ” ìƒìˆ˜ì´ë‹¤.

![algorithm1](../pics/bn_algorithm1.png)

The BN transform can be added to a network to manipulate any activation.