# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift : 2015

## 0. Abstract
Training Deep Neural Networks is complicated by the fact
that the distribution of each layerâ€™s inputs changes during
training, as the parameters of the previous layers change.

> ì‹¬ì¸µ ì‹ ê²½ë§ì˜ í›ˆë ¨ì€ ì´ì „ layerì˜ ë§¤ê°œ ë³€ìˆ˜ê°€ ë³€ê²½ë¨ì— ë”°ë¼, í›ˆë ¨ ì¤‘ì— ê° layerì˜ ì…ë ¥ ë¶„í¬ê°€ ë³€ê²½ëœë‹¤ëŠ” ì‚¬ì‹¤ ë•Œë¬¸ì— ë³µì¡í•˜ë‹¤.

This slows down the training by requiring lower learning
rates and careful parameter initialization, and makes it `notoriously`
hard to train models with `saturating nonlinearities`.

> learning rateë¥¼ ë‚®ì¶”ê±°ë‚˜ ì´ˆê¸° íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ í•˜ë©´, í•™ìŠµì†ë„ê°€ ëŠ¦ìœ¼ì§€ë¨¸,
> ì´ ê²½ìš°ëŠ” í¬í™” ë¹„ì„ í˜•ì„±ì„ ê°€ì§„ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ì–´ë µê²Œ ë§Œë“ ë‹¤.

We refer to this phenomenon as `internal covariate
shift`, and address the problem by normalizing layer inputs.

> ìš°ë¦¬ëŠ” ì´ ìƒí™©ì„ `internal covariate shift` ë¼ê³  ë¶€ë¥´ë©°, ë ˆì´ì–´ì˜ ì…ë ¥ê°’ì„ ì •ê·œí™”
> í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

Our method draws its strength from making normalization
a part of the model architecture and performing the
normalization for each training mini-batch.

> ìš°ë¦¬ì˜ ë°©ì‹ì€ ëª¨ë¸ì˜ ê° íŒŒíŠ¸ì™€, ê°ê°ì˜ ë¯¸ë‹ˆë°°ì¹˜ë‹¨ìœ„ë“¤ì„ ì •ê·œí™”ë¥¼ ì§„í–‰í•œë‹¤.

Batch Normalization allows us to use much higher learning rates and
be less careful about initialization.

> ë°°ì¹˜ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ë†’ì€ learning rateë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, 
> ì´ˆê¸°ê°’ ìƒì„±ì— ëœ ì‹ ê²½ì„ ì“°ê²Œ í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
 
It also acts as a regularizer, in some cases eliminating the need for Dropout.

> ì´ê²ƒì€ ë˜í•œ ì •ê·œí™”ì˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨, DropuOutì˜ í•„ìš”ì„±ì„ ì¤„ì—¬ì¤€ë‹¤.

Applied to a state-of-the-art image classification model,
Batch Normalization achieves the same accuracy with 14
times fewer training steps, and `beats the original model`
by a `significant margin`. 

> SOTAë¥¼ ë‹¬ì„±í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ëª¨ë¸ì— ì ìš©í•¨ìœ¼ë¡œì¨, ê°™ì€ ì •í™•ë„ì§€ë§Œ 14ë²ˆì˜ í•™ìŠµ stepì„ ì¤„ì˜€ìœ¼ë©°,
> ê·¸ë¦¬ê³  ê¸°ì¡´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ ì•ì§ˆë €ìŠµë‹ˆë‹¤.

Using an ensemble of batch normalized networks, we improve upon the best published
result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.

> ë°°ì¹˜ì •ê·œí™” ì•™ìƒë¸”ì„ ì ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ImageNet ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.
> : top-5 ê²€ì¦ë°ì´í„°ì…‹ì˜ errorê°€ 4.9%ë¡œ ì¸ê°„ì´ í‰ê°€í•˜ëŠ” ê²ƒì„ ë›°ì–´ë„˜ì—ˆë‹¤.

---
## 1. Introduction


Deep learning has dramatically advanced the state of the art in vision, speech, and many other areas.

> ë”¥ëŸ¬ë‹ì€ ë“œë¼ë§ˆí‹±í•˜ê²Œ ë¹„ì ¼, ìŒì„± ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë°œì „ì¤‘ì´ë‹¤.

Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum (Sutskever et al., 2013) and Adagrad (Duchi et al., 2011) have been used to achieve state of the
art performance. 

> í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ì€ ê¹Šì€ ì‹ ê²½ë§ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ”ë°ì— ì¦ëª…ë˜ì—ˆìœ¼ë©°
> ê·¸ë¦¬ê³  ëª¨ë©˜í…€, AdaGrad ë“±ê³¼ ê°™ì€ SGDì˜ ë³€í˜•ì´ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.

SGD optimizes the parameters $\Theta$ of the network, so as to minimize the loss
> SGD ë„¤íŠ¸ì›Œí¬ì˜ ê²½ì‚¬($\Theta$)ë¥¼ ìµœì í™”í•˜ë©° loss ê°’ì„ ì¤„ì¸ë‹¤.

![introduct_math1](../pics/bn_math1.png)

where x1...N is the training data set. 

> x1, ... Nì€ í•™ìŠµ ë°ì´í„°ì…‹ì´ë‹¤.

With SGD, the training proceeds in steps, and at each step we consider a minibatch x1...m of size m. 

> SGDë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, í•™ìŠµì€ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰ë˜ë©° ìš°ë¦¬ëŠ” ë¯¸ë‹ˆë°°ì¹˜ x1 ë¶€í„° mê¹Œì§€ì˜ ë‹¨ê³„ë¥¼ ê³ ë ¤í•˜ë‚Ÿ.

The mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing

> ë¯¸ë‹ˆë°°ì¹˜ëŠ” ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì ì˜ ê²½ì‚¬ì— ê·¼ì ‘í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. ì•„ë˜ì™€ ê°™ì€ ì—°ì‚°ì„ í†µí•´ì„œ..

![introduct_math2](../pics/bn_math2.png)

Using mini-batches of examples, as `opposed` to one example
at a time, is helpful in several ways.

> ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë‹¤ì–‘í•œ ë°©ë©´ì— ë„ì›€ì„ ì¤€ë‹¤.

First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. 

> ì²«ì§¸, ë¯¸ë‹ˆ ë°°ì¹˜ì— ëŒ€í•œ ì†ì‹¤ì˜ ê¸°ìš¸ê¸°ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ í’ˆì§ˆì´ í–¥ìƒë˜ëŠ” í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•œ ê¸°ìš¸ê¸°ì˜ ì¶”ì •ì¹˜ì´ë‹¤.

Second, computation over a batch can be much more efficient than m computations for individual examples, due to the parallelism afforded by the modern computing platforms.

> ë‘˜ì§¸, ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ìµœê·¼ ê³„ì‚° í”Œë«í¼ë“¤ì´ ë³‘ë ¬ì²˜ë¦¬ë¥¼ í—ˆìš©í•˜ê¸° ë•Œë¬¸ì—, ê°ê°ì„ më²ˆ ê³„ì‚°í•˜ëŠ”ê²ƒë³´ë‹¤ ë” íš¨ìœ¨ì ì´ë‹¤.
---

While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. 

> ë°˜ë©´, SGDëŠ” ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì´ì§€ë§Œ, ê·¸ê²ƒì€ hyper parameter íŠœë‹ì„ ì •êµí•˜ê²Œ í•´ì•¼í•˜ë©°, í•™ìŠµìœ¨ë˜í•œ ì •ê·œí•˜ê²Œ ì‘ì—…í•´ì•¼í•œë‹¤. ë˜í•œ ì´ˆê¸°ê°€ì¤‘ì¹˜ë„ ê·¸ë ‡ë‹¤.

The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers â€“ so that small changes to the network parameters amplify as the network becomes deeper.

> ê·¸ í•™ìŠµì€ ëª¨ë“  inputì´ ê°ê°ì˜ layerê°€ ì§„í–‰ë ëŒ€ë§ˆë‹¤ ì˜í–¥ì„ ë¼ì¹˜ê¸° ë•Œë¬¸ì—, ê¹Šì–´ì§ˆìˆ˜ë¡ ì˜í–¥ì´ í¬ë‹¤.

The change in the distributions of layersâ€™ inputs presents a problem because the layers need to continuously adapt to the new distribution. 

> ê° ì¸µì˜ ë ˆì´ì–´ë“¤ì€ ìƒˆë¡œìš´ ë¶„í¬ì— ëŒ€í•œ ë³€í™”ë¥¼ í•„ìš”ë¡œ í•œë‹¤.

When the input distribution to a learning system changes, it is said to experience `covariate shift` (Shimodaira, 2000). 

> í•™ìŠµ ë•Œ, ì…ë ¥ê°’ì˜ ë¶„í¬ê°€ ë³€í•œë‹¤ë©´ covariate shift(ê³µë³€ëŸ‰ ì´ë™)ë¥¼ ê²ªëŠ”ë‹¤.

This is typically handled via `domain adaptation`(Jiang, 2008). 

> ì´ê²ƒì€ ë„ë©”ì¸ ì ì‘(?)ì„ í†µí•´ í•´ê²°ëœë‹¤.

However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. 

> ê·¸ëŸ¬ë‚˜ ê³µë³€ëŸ‰ ì´ë™ì˜ ê°œë…ì€ í•™ìŠµ ì‹œìŠ¤í…œ ì „ì²´ ë¿ ì•„ë‹ˆë¼, í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ ë° ë ˆì´ì–´ì—ë„ ì˜í–¥ì„ ë¼ì¹œë‹¤.

Consider a network computing
> ì•„ë˜ì™€ ê°™ì€ network ì—°ì‚°ì„ ê³ ë ¤í•œë‹¤.

![introduct_math3](../pics/bn_math3.png)

where F1 and F2 are arbitrary transformations, and the parameters â€€$\Theta_1, \Theta_2$ are to be learned so as to minimize the loss â„“. 

> F1, F2ëŠ” ì„ì˜ì˜ ë³€í˜•ê°’ì´ë©°, íŒŒë¼ë¯¸í„° $\Theta_1, \Theta_2$ ëŠ” loss $\ell$ ì„ ìµœì†Œí™”ì‹œí‚¤ë„ë¡ í•™ìŠµí•œë‹¤.

Learning $\Theta_2$ can be viewed as if the inputs $x = F_1(u,â€€\Theta_1)$ are fed into the sub-network
> $\Theta_2$ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ $x = F_1(u,â€€\Theta_1)$ë¥¼ inputìœ¼ë¡œ í•˜ì—¬, í•˜ìœ„ ê³„ì¸µ ë„¤íŠ¸ì›Œí¬ì— ì…ë ¥ë˜ëŠ”ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

![introduct_math4](../pics/bn_math4.png)

For example, a gradient descent step
> ê²½ì‚¬ í•˜ê°•ë²• ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![introduct_math5](../pics/bn_math5.png)


(for batch size $m$ and learning rate Î±) is exactly equivalent to that for a `stand-alone network` $F_2$ with input $x$. 

> ë°°ì¹˜ì‚¬ì´ì¦ˆ $m$ê³¼ í•™ìŠµë¥  $\alpha$ëŠ” ì •í™•íˆ stand-alone ë„¤íŠ¸ì›Œí¬ì¸ $x$ë¥¼ inputìœ¼ë¡œ í•˜ëŠ” $F_2$ì™€ ì •í™•íˆ ë™ì¼í•˜ë‹¤.

Therefore, the input distribution properties that make training more efficient â€“ such as having the same distribution between the training and test data â€“ apply to training the sub-network as well. 

> ë”°ë¼ì„œ input ë°ì´í„°ì˜ ë¶„í¬ íŠ¹ì„±ì€ í•™ìŠµì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“ ë‹¤. such as)í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ì˜ ë¶„í¬ê°€ ê°™ì„ ê²½ìš° - í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ì—ë„ ì˜ ì ìš©ëœë‹¤.

As such it is advantageous for the distribution of $x$ to remain fixed over time. 

> ë”°ë¼ì„œ, $x$ì˜ ë¶„í¬ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë”ë¼ë„ ìœ ì§€ë˜ëŠ”ê²ƒì´ ìœ ë¦¬í•˜ë‹¤. 

Then, $\Theta_2$ does not have to readjust to compensate for the change in the distribution of x.
> ê·¸ë ‡ê²Œëœë‹¤ë©´, $\Theta_2$ëŠ” $x$ì˜ ë¶„í¬ ì¡°ì •ì„ ìœ„í•´ ì¬ì¡°ì • í•  í•„ìš”ê°€ ì—†ë‹¤.

Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the subnetwork, as well.

> ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•œ ì…ë ¥ì˜ ê³ ì •ëœ ë¶„í¬ëŠ” í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ ë°–ì˜ ê³„ì¸µë“¤ì—ê²Œë„ ê¸ì •ì ì¸ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì´ë‹¤.

Consider a layer with a sigmoid activation function $z = g(Wu + b)$ where $u$ is the layer input, the weight matrix $W$ and bias vector $b$ are the layer parameters to be learned, and $g(x) = \frac{1}{1+exp(âˆ’x)}$. As $|x|$ increases, $gâ€²(x)$ tends to zero.

> ìœ„ì™€ ê°™ì€ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”í•¨ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬, $u$ê°€ ë ˆì´ì–´ì˜ inputì„ì„ ê³ ë ¤í•˜ë©´ , $W$ì™€ $b$ëŠ” ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ì—ì˜í•´ í•™ìŠµë ê²ƒì´ë©°, $|x|$ ê°€ ë†’ì„ìˆ˜ë¡ ê²½ì‚¬ëŠ” 0ì— ê·¼ì‚¬í•˜ê²Œ ë  ê²ƒì´ë‹¤.


This means that for all dimensions of $x = Wu+b$ except those with small absolute values, the gradient flowing down to $u$ will vanish and the model will train slowly.

> ì´ê²ƒì€ ëª¨ë“  ì°¨ì›ì´ $x$ì˜ ì ˆëŒ€ê°’ì„ ì‚¬ìš©í•˜ë©´, ì ˆëŒ€ê°’ì´ ì‘ì€ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” $u$ëŠ” ì†Œë©¸ë  ê²ƒì´ë‹¤.

However, since $x$ is affected by $W, b$ and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of $x$ into the `saturated regime` of the nonlinearity and slow down the convergence

> ê·¸ëŸ¬ë‚˜ xëŠ” W, bì™€ ì•„ë˜ì˜ ëª¨ë“  ì¸µì˜ íŒŒë¼ë¯¸í„°ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ì— xì˜ ë§ì€ ì°¨ì›ì„ ë¹„ì„ í˜•ì„±ì˜ í¬í™”ìƒíƒœë¡œ ì´ë™ì‹œí‚¤ê³  ìˆ˜ë ´ì„ ëŠ¦ì¶œ ê²ƒì´ë‹¤.


This effect is amplified as the network depth increases. 

> ì´ëŸ¬í•œ íš¨ê³¼ëŠ” ë„¤íŠ¸ì›Œí¬ ê¹Šì´ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì¦í­ë©ë‹ˆë‹¤.

In practice, the `saturation problem` and the resulting vanishing gradients are usually addressed by using `Rectified Linear Units` (Nair & Hinton, 2010) ReLU(x) = max(x, 0), careful initialization (Bengio & Glorot, 2010; Saxe et al., 2013), and small learning rates. 

> ê²½ì‚¬ ì†Œë©¸ ë° ê¸‰ê²©í•˜ê²Œ ê²½ì‚¬ê°€ ë³€í™”ëŠ” ë¬¸ì œëŠ” ReLU í™œì„±í™”í•¨ìˆ˜ë¥¼ í†µí•´ ì–´ëŠì •ë„ í•´ê²°ì´ ë˜ì—ˆì—ˆë‹¤. ë˜í•œ ì´ˆê¸°ê°’ì„ ë‹¤ë£¨ê±°ë‚˜ learning rateë¥¼ ë°”ê¾¸ëŠ” ë°©ì‹ìœ¼ë¡œë„.

If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate.

> ê·¸ëŸ¬ë‚˜ ë¹„ì„ í˜•ì„±ì¸ Input ì´ ì¢€ë” ì•ˆì •ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì— ë“¤ì–´ì™€ì„œ í•™ìŠµì´ ëœë‹¤ë©´, í¬í™”ìƒíƒœì— ë¹ ì§ˆ ê°€ëŠ¥ì„±ì´ ë‚®ì•„ì§€ê³ , í›ˆë ¨ì†ë„ ë˜í•œ ê°€ì†í™”ë  ê²ƒì´ë‹¤.

We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as `Internal Covariate Shift`. 

> ìš°ë¦¬ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ ë…¸ë“œ ë¶„í¬ì˜ ë³€í™”ë¥¼ ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì´ë¼ê³  í•œë‹¤.

Eliminating it offers a promise of faster training. 

> ì´ê²ƒì„ ì œê±°í•¨ìœ¼ë¡œì¨ í•™ìŠµì†ë„ê°€ ë¹¨ë¼ì§„ë‹¤.

<font color='red'>We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets.  </font>

> ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ ì¤„ì´ê³ , ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ì‹¬ì¸µ ì‹ ê²½ë§ì˜ í›ˆë ¨ì„ íšê¸°ì ìœ¼ë¡œ ê°€ì†í™”í•˜ëŠ” ë°°ì¹˜ ì •ê·œí™”ë¼ê³  ë¶ˆë¦¬ëŠ” ìƒˆë¡œìš´ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•œë‹¤.


It accomplishes this via a normalization step that fixes the means and variances of layer inputs. 

> ì´ê²ƒì€ ê³„ì¸µ ì…ë ¥ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ìˆ˜ì •í•˜ëŠ” ì •ê·œí™” ë‹¨ê³„ë¥¼ í†µí•´ ë‹¬ì„±ëœë‹¤.

Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values.

> ë°°ì¹˜ ì •ê·œí™”ëŠ” ë˜í•œ ë§¤ê°œ ë³€ìˆ˜ì˜ ì²™ë„ ë˜ëŠ” ì´ˆê¸° ê°’ì— ëŒ€í•œ ê·¸ë ˆì´ë””ì–¸íŠ¸ì˜ ì˜ì¡´ì„±ì„ ì¤„ì„ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ê·¸ë ˆì´ë””ì–¸íŠ¸ íë¦„ì— ìœ ìµí•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

This allows us to use much higher learning rates without the risk of divergence. 

> ì´ê²ƒì€ í•™ìŠµìœ¨ì„ ë†’ì—¬ë„, ê²½ì‚¬ê°€ ë°œì‚°í•˜ëŠ” ë¬¸ì œê°€ ì—†ë„ë¡ í•œë‹¤.

Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014).

> ê²Œë‹¤ê°€, ë°°ì¹˜ì •ê·œí™”ëŠ” ëª¨ë¸ì— ê·œì œë¥¼ í•´ì£¼ë©°, DropOutì˜ í•„ìš”ì„±ì„ ì¤„ì—¬ì¤€ë‹¤.

Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.

> ë§ˆì§€ë§‰ìœ¼ë¡œ, ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ë¹„ì„ í˜•ì ì¸ í™œì„±í™”í•¨ìˆ˜ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤??


In Sec. 4.2, we apply Batch Normalization to the bestperforming ImageNet classification network, and show that we can match its performance using only 7% of the training steps, and can further exceed its accuracy by a `substantial margin`

> 4.2ì ˆì—ì„œëŠ” Batch Normalizationì„ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ImageNet ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ì— ì ìš©í•˜ê³  í•™ìŠµ ë‹¨ê³„ì˜ 7%ë§Œ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìœ¼ë©° ì •í™•ë„ë„ í›¨ì”¬ ëŠ¥ê°€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

Using an ensemble of such networks trained with Batch Normalization, we achieve the top-5 error rate that improves upon the best known results on ImageNet classification.

> ë°°ì¹˜ì •ê·œí™” ì•™ìƒë¸”ì„ ì ìš©í•˜ì—¬, ìš°ë¦¬ëŠ” ImageNet ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.
> : top-5 ê²€ì¦ë°ì´í„°ì…‹ì˜ errorê°€ 4.9%ë¡œ ì¸ê°„ì´ í‰ê°€í•˜ëŠ” ê²ƒì„ ë›°ì–´ë„˜ì—ˆë‹¤.

## 2. Towards Reducing Internal Covariate Shift

We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. 

> ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ í›ˆë ¨ ì¤‘ ë„¤íŠ¸ì›Œí¬ ë§¤ê°œ ë³€ìˆ˜ì˜ ë³€í™”ë¡œ ì¸í•œ ë„¤íŠ¸ì›Œí¬ í™œì„±í™” ë¶„í¬ì˜ ë³€í™”ë¡œ ì •ì˜í•œë‹¤.

To improve the training, we seek to reduce the internal covariate shift.

> í›ˆë ¨ì„ ê°œì„ í•˜ê¸° ìœ„í•˜ì—¬,  ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ ì¤„ì´ëŠ” ë°©ë²•ì„ íƒìƒ‰í•œë‹¤.

By fixing the distribution of the layer inputs x as the training progresses, we expect to improve the training speed.

> í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ë ˆì´ì–´ ì…ë ¥ xì˜ ë¶„í¬ë¥¼ ê³ ì •í•¨ìœ¼ë¡œì¨ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are `whitened` â€“ i.e., linearly transformed to have zero means and unit variances, and `decorrelated`. 

> ì…ë ¥ë°ì´í„°ê°€ whitend(? : ì•„ë§ˆ í‘œì¤€í™”ê°™ì€ê°œë…) ìˆ˜ë ´ì†ë„ê°€ ë” ë¹¨ë¼ì§„ë‹¤ëŠ” ê²ƒì€ ì´ë¯¸ ì˜ ì•Œë ¤ì ¸ìˆë‹¤. - í‰ê·  0ì„ ê°–ìœ¼ë©°, ë‹¨ìœ„ ë¶„ì‚°ê³¼ ë¹„ìƒê´€ì„±ì„ ê°–ëŠ”ë‹¤. 

As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer.

> ê° ì¸µì´ ì•„ë˜ ì¸µì— ì˜í•´ ìƒì„±ëœ ì…ë ¥ì„ ê´€ì°°í•˜ê¸° ë•Œë¬¸ì— ê° ì¸µì˜ ì…ë ¥ì— ëŒ€í•´ ë™ì¼í•œ í™”ì´íŠ¸ë‹ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•  ê²ƒì´ë‹¤.

By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.

> ê° ì¸µì— ëŒ€í•œ ì…ë ¥ì„ whitening í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì˜ ë¶€ì‘ìš©ì„ ì œê±°í•˜ê¸° ìœ„í•´,  ì…ë ¥ì˜ ê³ ì •ëœ ë¶„í¬ë¥¼ ìœ„í•´ ë‹¨ê³„ë¥¼ ì§„í–‰í• ê²ƒì´ë‹¤.


We could consider whitening activations at every training step or at some interval, either by __modifying the
network directly__ or by __changing the parameters of the optimization algorithm__ to depend on the `network activation values` (Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins & Kavukcuoglu).

> ìš°ë¦¬ëŠ” networkë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ê±°ë‚˜, íŒŒë¼ë¯¸í„° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ë³€ê²½í•¨ìœ¼ë¡œì¨ ê° ë‹¨ê³„ë³„ whitening í™œì„±í™”ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤. 
> ì´ëŠ” network activation valueì— ì˜ì¡´ì ì´ë‹¤. (í™œì„±í™”í•¨ìˆ˜ ë§í•˜ëŠ”ê±´ì§€?)

However, if these modifications are `interspersed` with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect
of the gradient step

> ê·¸ëŸ¬ë‚˜ ì´ ê²½ìš° ë“œë¬¸ë“œë¬¸ ìµœì í™” ë‹¨ê³„ê°€ ìˆë‹¤ë©´, ê²½ì‚¬ í•˜ê°• ë‹¨ê³„ëŠ” ì •ê·œí™”ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ë ¤ê³  ì‹œë„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê²½ì‚¬ ë‹¨ê³„ì˜ ì˜í–¥ì„ ê°ì†Œì‹œí‚¨ë‹¤.

For example, consider a layer with the __input $u$__ that adds the learned bias $b$, and normalizes the result by subtracting the mean of the activation computed over the training data: $\hat{x} = x âˆ’ E[x]$ where $x = u + b$, $X = \{x1...N\}$ is the set of values of $x$ over the training set, and $E[x] = \frac{1}{N}\sum_i^N xi$.

> ë‹¨ê³„ë³„ë¡œ input dataì— ëŒ€í•´ì„œ ì •ê·œí™” ê³¼ì •ì„ ì§„í–‰í•˜ëŠ”ê²ƒìœ¼ë¡œ í•´ì„ë¨.

If a gradient descent step ignores the dependence of $E[x]$ on $b$, then it will update $b \leftarrow b + \triangle b$, where â€€$\triangle b \propto âˆ’âˆ‚â„“/âˆ‚ \hat{x}$
> ë§Œì•½ ê²½ì‚¬í•˜ê°•ë²•ì´ ê¸°ëŒ€ê°’$E[x]$ ì™€ $bias$ë¥¼ ë¬´ê¸°í•œë‹¤ë©´ ìœ„ì™€ ê°™ì´ ì—…ë°ì´íŠ¸ ë  ê²ƒì´ë‹¤.

Then $u+(b+\triangle b)-E[u+(b+\triangle b)]=u+b-E[u+b]$.
> ìœ„ì™€ ê°™ì´ ì—…ë°ì´íŠ¸ê°€ ëì„ ê²ƒì´ë‹¤?

Thus, the combination of the update to b and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. 

> ê·¸ëŸ¬ë¯€ë¡œ, $b$ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ì™€ ê·¸ì— ë”°ë¥¸ ì •ê·œí™” ë³€ê²½ì˜ ì¡°í•©ì€ ê³„ì¸µ ì¶œë ¥ì˜ ë³€í™”ë„, ê²°ê³¼ì ìœ¼ë¡œ ì†ì‹¤ë„ ì´ˆë˜í•˜ì§€ ì•Šì•˜ë‹¤.

As the training continues, $b$ will grow indefinitely while the loss remains fixed.

> í›ˆë ¨ì´ ê³„ì†ë ìˆ˜ë¡ $b$ëŠ” ì†ì‹¤ì´ ê³ ì •ëœ ìƒíƒœì—ì„œ ë¬´í•œ ì„±ì¥í•œë‹¤.

This problemcan get worse if the normalization not only centers but also scales the activations.
> ì´ ë¬¸ì œëŠ” ì •ê·œí™”ê°€ ì¤‘ì‹¬ì¼ ë¿ë§Œ ì•„ë‹ˆë¼ í™œì„±í™” í•¨ìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§ í•œë‹¤ë©´, ë” ì•…í™”ë  ìˆ˜ ìˆë‹¤.

We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step.

> ì •ê·œí™” ë§¤ê°œ ë³€ìˆ˜ê°€ ê²½ì‚¬ í•˜ê°• ë‹¨ê³„ ë°–ì—ì„œ ê³„ì‚°ë  ë•Œ ëª¨ë¸ì´ í­íŒŒë˜ëŠ” ì´ˆê¸° ì‹¤í—˜ì—ì„œ ì´ë¥¼ ê²½í—˜ì ìœ¼ë¡œ ê´€ì°°í–ˆë‹¤.

The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. 

> ìœ„ì˜ ì ‘ê·¼ë²•ì˜ ë¬¸ì œëŠ” ê²½ì‚¬ í•˜ê°• ìµœì í™”ëŠ” ì •ê·œí™”ê°€ ì¼ì–´ë‚œë‹¤ëŠ” ì‚¬ì‹¤ì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.


<font color='red'>To address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution.  </font>

> ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ ê°’ì— ëŒ€í•´ ë„¤íŠ¸ì›Œí¬ê°€ í•­ìƒ ì›í•˜ëŠ” ë¶„í¬ë¡œ í™œì„±í™”ë¥¼ ìƒì„±í•˜ë„ë¡ ë³´ì¥í•˜ê³ ì í•œë‹¤.


Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters $\Theta$.

> ê·¸ë ‡ê²Œ í•˜ëŠ” ê²ƒì€ ì •ê·œí™”ì™€ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ $\Theta$ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤ì˜ ê¸°ìš¸ê¸°ë¥¼ í—ˆìš©í•  ê²ƒì´ë‹¤.

Let again x be a layer input, treated as a vector, and X be the set of these inputs over the training data set. 

> ë‹¤ì‹œ xë¥¼ ë²¡í„°ë¡œ ì·¨ê¸‰í•˜ëŠ” ë ˆì´ì–´ ì…ë ¥ìœ¼ë¡œ í•˜ê³  Xë¥¼ í›ˆë ¨ ë°ì´í„° ì§‘í•©ì— ëŒ€í•œ ì´ëŸ¬í•œ ì…ë ¥ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ í•˜ì.

The normalization can then be written as a transformation
> ê·¸ ì •ê·œí™”ì˜ ë³€í™˜ì€ ì•„ë˜ì‹ì²˜ëŸ¼ ì“¸ ìˆ˜ ìˆë‹¤.

![introduct_math6](../pics/bn_math6.png)

which depends not only on the given training example $x$ but on all examples $\chi$ â€“ each of which depends on $\Theta$ if $x$ is generated by another layer. For backpropagation, we would need to compute the `Jacobians`

> ì´ëŠ” ì£¼ì–´ì§„ í•™ìŠµ ì˜ˆì œ ğ‘¥ ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  ì˜ˆì œ ğœ’ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ”ë°, ê° ì˜ˆì œ ğœ’ëŠ” $x$ê°€ ë‹¤ë¥¸ ê³„ì¸µì— ì˜í•´ ìƒì„±ë  ê²½ìš° Î˜ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. 
> ì—­ì „íŒŒë¥¼ ìœ„í•´ì„œëŠ” ìì½”ë¹„ì•ˆë“¤ì„ ê³„ì‚°í•œë‹¤.

![introduct_math7](../pics/bn_math7.png)

ignoring the latter term would lead to the explosion described above. 
> í›„ìì˜ ìš©ì–´ë¥¼ ë¬´ì‹œí•˜ë©´, ìœ„ì—ì„œ ì„¤ëª…í•œ ê¸°ìš¸ê¸° í­ë°œì´ ì¼ì–´ë‚  ê²ƒì´ë‹¤.

Within this framework, whitening the layer inputs is expensive, as it requires computing the `covariance matrix` $Cov[x] = Ex_{âˆˆ\chi} [xxT ] âˆ’ E[x]E[x]^T$ and its inverse square root, to produce the `whitened activations` $Cov[x]^{-1/2}(x âˆ’ E[x])$, as well as the derivatives of these transforms for backpropagation. 

> ì´ í”„ë ˆì„ì›Œí¬ì—ì„œëŠ”, layer inputì„ whkiteningí•˜ëŠ” ì—°ì‚°ì€ ë¹„ì‹¸ë‹¤. ìš°ë¦¬ëŠ” ì—°ì‚°ì„ ìœ„í•´ ê³µë¶„ì‚°í–‰ë ¬ì„ í•„ìš”ë¡œ í•˜ë©°, ê·¸ê²ƒì€ ì—­ìˆ˜ ë° ë£¨íŠ¸ë¥¼ ì”Œì›Œì„œ `whitend activations`ë¥¼ ë§Œë“ ë‹¤. ìš°ë¦¬ëŠ” ì—­ì „íŒŒë¥¼ ìœ„í•´ ì´ê²ƒì„ íŒŒìƒí•œë‹¤.

This motivates us to seek an alternative that performs input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update.

> ì´ê²ƒì€ ìš°ë¦¬ê°€ ì°¨ë³„í™” ê°€ëŠ¥í•˜ê³  ëª¨ë“  ë§¤ê°œ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ í›„ ì „ì²´ í•™ìŠµë°ì´í„°ì˜ ë¶„ì„ì„ ìš”êµ¬í•˜ì§€ ì•ŠëŠ” ë°©ì‹ìœ¼ë¡œ ì…ë ¥ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì•ˆì„ ì°¾ë„ë¡ ë§Œë“ ë‹¤.

Some of the previous approaches (e.g. (Lyu & Simoncelli, 2008)) use statistics computed over a single training example, or, in the case of image networks, over different feature maps at a given location.

> ì´ì „ ì ‘ê·¼ë²• ì¤‘ ì¼ë¶€ëŠ” ë‹¨ì¼ í›ˆë ¨ ì˜ˆì‹œì— ëŒ€í•´, ë˜ëŠ” ì´ë¯¸ì§€ ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìš° ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ ë‹¤ë¥¸ íŠ¹ì§•ë§µì— ëŒ€í•´ ê³„ì‚°ëœ í†µê³„ê°’ ì‚¬ìš©í•œë‹¤.

However, this changes the representation ability of a network by discarding the absolute scale of activations.
> ê·¸ëŸ¬ë‚˜ ì´ëŠ” í™œì„±í™”ì˜ ì ˆëŒ€ì ì¸ scaleì„ ë²„ë¦¼ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ì •ë³´ë¥¼ ë³€í™”ì‹œí‚¨ë‹¤.

We want to a preserve the information in the network, by normalizing the activations in a training example relative to the statistics of the entire training data.
> ìš°ë¦¬ëŠ” ì „ì²´ í›ˆë ¨ ë°ì´í„°ì˜ í†µê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í›ˆë ¨ ì˜ˆì œì˜ í™œì„±í™”ë¥¼ ì •ê·œí™”í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ì˜ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³ ì í•œë‹¤.

##  3. Normalization via Mini-Batch Statistics

Since the full whitening of each layerâ€™s inputs is costly and not everywhere differentiable, we make two necessary simplifications. 

> ê° ì¸µì˜ ì…ë ¥ì— ëŒ€í•œ ì™„ì „í•œ í™”ì´íŠ¸ë‹ì€ ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì–´ë””ì—ì„œë‚˜ êµ¬ë³„í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ í•„ìš”í•œ ë‹¨ìˆœí™”ë¥¼ í•œë‹¤.

The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have the mean of zero and the variance of 1. 

> ì²« ë²ˆì§¸ëŠ” ë ˆì´ì–´ ì…ë ¥ê³¼ ì¶œë ¥ì˜ íŠ¹ì§•ì„ ê³µë™ìœ¼ë¡œ í™”ì´íŠ¸ë‹í•˜ëŠ” ëŒ€ì‹ , ìš°ë¦¬ëŠ” ê° ìŠ¤ì¹¼ë¼ íŠ¹ì§•ì„ 0ê³¼ 1ì˜ ë¶„ì‚°ìœ¼ë¡œ ë§Œë“¤ì–´ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”í•  ê²ƒì´ë‹¤.

For a layer with $d$-dimensional input $x = (x^{(1)} . . . x^{(d)})$, we will normalize each dimension

> ìœ„ì™€ê°™ì€ $d$ ì°¨ì›ì˜ inputì´ ë“¤ì–´ì˜¨ë‹¤ë©´, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ê°™ì´ ê°ê°ì˜ ì°¨ì›ì€ ì •ê·œí™”ì‹œí‚¬ê²ƒì´ë‹¤.

![introduct_3_math1](../pics/bn3_math1.png)


where the expectation and variance are computed over the training data set. 

> ì—¬ê¸°ì„œ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê¸°ëŒ€ê°’ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

As shown in (LeCun et al., 1998b), such normalization speeds up convergence, even when the features are not decorrelated.

> (LeCun et al., 1998b)ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì´ëŸ¬í•œ ì •ê·œí™”ëŠ” íŠ¹ì§•ì´ ìƒê´€ê´€ê³„ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ìˆ˜ë ´ ì†ë„ë¥¼ ë†’ì¸ë‹¤.

Note that simply normalizing each input of a layer may change what the layer can represent. 

> ë‹¨ìˆœíˆ ê³„ì¸µì˜ ê° ì…ë ¥ì„ ì •ê·œí™”í•˜ëŠ” ê²ƒì€ ê³„ì¸µì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ê²ƒì„ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì£¼ëª©í•˜ë¼.

For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. 

> ì˜ˆë¥¼ ë“¤ì–´, ì‹œê·¸ëª¨ì´ë“œì˜ ì…ë ¥ì„ ì •ê·œí™”í•˜ë©´ ë¹„ì„ í˜•ì„±ì˜ ì„ í˜•ìœ¼ë¡œ ì œí•œëœë‹¤.

To address this, we make sure that the transformation inserted in the network can represent the `identity transform`.
> ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë„¤íŠ¸ì›Œí¬ì— ì‚½ì…ëœ ë³€í™˜ì´ identity transform(? : ê¸°ë³¸ ì •ë³´ë¡œ ì¶”ì¸¡) ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.

To accomplish this, we introduce, for each activation $x^{(k)}$, a pair of parameters $Î³^{(k)}, Î²^{(k)}$, which scale and shift the normalized value:

> ê°ê°ì˜ ëª¨ë“  í™œì„±í™”í•¨ìˆ˜ $x^{(k)}$ì— ëŒ€í•´, ê°ê°ì˜ ìŒì¸ $Î³^{(k)}, Î²^{(k)}$,ëŠ” ìŠ¤ì¼€ì¼ë§ ë° ì •ê·œí™”ê°’ì„ shiftí•œë‹¤.

![introduct_3_math2](../pics/bn3_math2.png)


![introduct_3_math3](../pics/bn3_math3.png)

> ì´ëŸ¬í•œ ë§¤ê°œ ë³€ìˆ˜ëŠ” ì›ë˜ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ì™€ í•¨ê»˜ í•™ìŠµë˜ê³  ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ ëŠ¥ë ¥ì„ ë³µì›í•œë‹¤.
> ìœ„ì™€ ê°™ì´ ì„¤ì •í•œë‹¤ë©´, ìš°ë¦¬ëŠ” ê¸°ë³¸ í™œì„±í™” ë¥¼ ë³µì›í•  ìˆ˜ ìˆë‹¤.


In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations.

> ê° í›ˆë ¨ ë‹¨ê³„ê°€ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë°°ì¹˜ ì„¤ì •ì—ì„œ ì „ì²´ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í™œì„±í™”ë¥¼ ì •ê·œí™”í•œë‹¤.

However, this is impractical when using stochastic optimization. 

> ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ í™•ë¥ ì  ìµœì í™”ë¥¼ ì‚¬ìš©í•  ë•Œ ë¹„í˜„ì‹¤ì ì´ë‹¤.

Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation.

> ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ë‘ ë²ˆì§¸ ë‹¨ìˆœí™”ë¥¼ í•œë‹¤ : í™•ë¥ ì  ê²½ì‚¬ í›ˆë ¨ì—ì„œ ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê° ë¯¸ë‹ˆ ë°°ì¹˜ëŠ” ê° í™œì„±í™”ì˜ í‰ê· ê³¼ ë¶„ì‚°ì— ëŒ€í•œ ì¶”ì •ì¹˜ë¥¼ ìƒì„±í•œë‹¤.

This way, the statistics used for normalization can fully participate in the gradient backpropagation. 

> ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”ì— ì‚¬ìš©ë˜ëŠ” í†µê³„ëŠ” ê·¸ë ˆì´ë””ì–¸íŠ¸ ì—­ì „íŒŒì— ì™„ì „íˆ ì°¸ì—¬í•  ìˆ˜ ìˆë‹¤.

Note that the use of minibatches is enabled by __computation of per-dimension variances__ rather than joint covariances; 

> ë¯¸ë‹ˆë°°ì¹˜ì˜ ì‚¬ìš©ì€ ì „ì²´ ê³µë¶„ì‚°ë³´ë‹¤ëŠ” ì°¨ì›ë‹¹ ë¶„ì‚° ê³„ì‚°ì— ì˜í•´ ê°€ëŠ¥í•˜ë‹¤.

in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices.

> ì´ ì „ì²´ ê²½ìš°, ë¯¸ë‹ˆ ì§„ë™ í¬ê¸°ê°€ í°ìƒ‰ìœ¼ë¡œ ë³€í•˜ëŠ” í™œì„±ì˜ ìˆ˜ë³´ë‹¤ ì‘ì„ ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì— ì •ê·œí™”ê°€ ìš”êµ¬ë˜ì–´ ë‹¨ìˆ˜ ê³µë¶„ì‚° í–‰ë ¬ì´ ìƒì„±ëœë‹¤.

Consider a mini-batch B of size m.

> í¬ê¸°ê°€ mì¸ ë¯¸ë‹ˆë°°ì¹˜ Bë¥¼ ê³ ë ¤í•´ë³´ì. 

Since the normalization is applied to each activation independently, let us focus on a particular activation $x^{(k)}$ and omit $k$ for clarity. 

>ì •ê·œí™”ëŠ” ê° í™œì„±í™”ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ë¯€ë¡œ íŠ¹ì • í™œì„±í™” $x^{(k)}$ì— ì´ˆì ì„ ë§ì¶”ê³  ëª…í™•ì„±ì„ ìœ„í•´ $k$ë¥¼ ìƒëµí•©ì‹œë‹¤.

We have $m$ values of this activation in the mini-batch,

>ìš°ë¦¬ëŠ” ë¯¸ë‹ˆë°°ì¹˜ì•ˆì—ì„œ $m$ ê°’ì˜ í™œì„±í™”ë¥¼ ê°–ëŠ”ë‹¤.

![introduct_3_math4](../pics/bn3_math4.png)

Let the normalized values be $\hat{x}_{1...m}$, and their linear transformations be $y_{1...m}$. 

> $\hat{x}_{1...m}$ ê°’ì„ ì •ê·œí™” í•˜ê³ , $y_{1...m}$ ë¡œ ì„ í˜•ë³€í™˜ í•˜ì.

We refer to the transform as the <font color='red'>Batch Normalizing Transform</font>. 

> ì•„ë˜ì™€ ê°™ì€ Batch Normalizing Transform ì„ ë³¼ ìˆ˜ ìˆë‹¤.

![introduct_3_math5](../pics/bn3_math5.png)

We present the BN Transform in Algorithm1. 

> ìš°ë¦¬ëŠ” Algoritm1ì— ë³€í™˜ê³¼ì •ì„ í‘œí˜„í•œë‹¤.

In the algorithm, $\epsilon$ is a constant added to the mini-batch variance for numerical stability.

> í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì—ì„œ $\epsilon$ëŠ” ìˆ˜ì¹˜ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë¯¸ë‹ˆ ë°°ì¹˜ ë¶„ì‚°ì— ì¶”ê°€ë˜ëŠ” ìƒìˆ˜ì´ë‹¤.

![algorithm1](../pics/bn_algorithm1.png)

The BN transform can be added to a network to manipulate any activation.

> BN ë³€í™˜ì€ í™œì„±í™”ë¥¼ ì¡°ì‘í•˜ê¸° ìœ„í•´ ë„¤íŠ¸ì›Œí¬ì— ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.

In the notation ğ‘¦=ğµğ‘ğ›¾,ğ›½(ğ‘¥), we  indicate that the parameters Î³ and Î² are to be learned,
but it should be noted that the BN transform does not 
independently process the activation in each training example

> ìœ„ ì‹ì—ì„œ. ë§¤ê°œ ë³€ìˆ˜ Î³ ì™€ Î²ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ì§€ë§Œ, 
> BN ë³€í™˜ì´ ê° í›ˆë ¨ ì˜ˆì œì˜ í™œì„±í™”ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì— ìœ ì˜í•´ì•¼ í•œë‹¤.

Rather, BNÎ³,Î²(x) depends both on the training
example and the other examples in the mini-batch. 

> BNÎ³,Î²(x) ëŠ” í•™ìŠµ ì˜ˆì‹œì™€ ë‹¤ë¥¸ ì˜ˆì‹œì˜ ë¯¸ë‹ˆë°°ì¹˜ì— ì˜ì¡´í•œë‹¤.

The  scaled and shifted values y are passed to other network layers.

> ì¶•ì²™ ë° ì´ë™ëœ ê°’ yëŠ” ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ ë„ë©´ì¸µì— ì „ë‹¬ë©ë‹ˆë‹¤.

The normalized activations ğ‘¥Ì‚ are internal to our
transformation, but their presence is crucial

> ì •ê·œí™”ëœ í™œì„±í™” ğ‘¥Ì‚ ëŠ” ìš°ë¦¬ ë³€í™˜ ë‚´ë¶€ì´ì§€ë§Œ, ê·¸ë“¤ì˜ ì¡´ì¬ëŠ” ì¤‘ìš”í•˜ë‹¤.

The distributions  of values of any ğ‘¥Ì‚  has the expected value of 0
and the variance of 1, as long as the elements of each
mini-batch are sampled from the same distribution, and
if we neglect ğœ– .

> ê° ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ì›ì†Œê°€ ë™ì¼í•œ ë¶„í¬ì—ì„œ ì¶”ì¶œë˜ê³ , ìš°ë¦¬ê°€ ğœ– ë¥¼ ë¬´ì‹œí•œë‹¤ë©´,
> ì„ì˜ì˜ ğ‘¥Ì‚ ê°’ì˜ ë¶„í¬ëŠ” 0ì˜ ê¸°ëŒ€ê°’ê³¼ 1ì˜ ë¶„ì‚°ì„ ê°–ëŠ”ë‹¤.

![img](../pics/bn3_math6.png)

> ì „ì²´ í•©ì€ 0, ì œê³±ì˜ í‰ê· ì€ 1ì„ ê¸°ëŒ€í•œë‹¤.
> ê°ê°ì˜ ì •ê·œí™” í™œì„±í™” ğ‘¥Ì‚ (ğ‘˜) ëŠ” ê·¸ ë‹¤ìŒì— ì›ë˜ ë„¤íŠ¸ì›Œí¬ì— ì˜í•´ ìˆ˜í–‰ë˜ëŠ” ë‹¤ë¥¸ ì²˜ë¦¬ë˜ëŠ” 
> í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ì˜ ì„ í˜•ë³€í™˜ êµ¬ì„±ìš”ì†Œë¡œ í™œìš©ëœë‹¤.


These sub-network  inputs all have fixed means and variances, and although
the joint distribution of these normalized ğ‘¥Ì‚ (ğ‘˜) can change
over the course of training, we expect that the introduction
of normalized inputs accelerates the training of the
sub-network and, consequently, the network as a whole.

> ì´ëŸ¬í•œ ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ì€ ëª¨ë‘ ê³ ì •ëœ ìˆ˜ë‹¨ê³¼ ë¶„ì‚°ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, 
> ì´ëŸ¬í•œ ì •ê·œí™”ëœ ğ‘¥Ì‚ (ğ‘˜) ì˜ ê³µë™ ë¶„í¬ê°€ í›ˆë ¨ ê³¼ì •ì— ê±¸ì³ ì§€ì†ë  ìˆ˜ ìˆì§€ë§Œ, 
> ì •ê·œí™”ëœ ì…ë ¥ì˜ ë„ì…ì€ ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ì„ ê°€ì†í™”í•˜ê³  ê²°ê³¼ì ìœ¼ë¡œ 
> ë„¤íŠ¸ì›Œí¬ ì „ì²´ì˜ í›ˆë ¨ì„ ê°€ì†í™”í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.

During training we need to backpropagate the gradient
of loss â„“ through this transformation, as well as compute
the gradients with respect to the parameters of the
BN transform. We use chain rule, as follows (before simplification):

> í›ˆë ¨ ë™ì•ˆ ìš°ë¦¬ëŠ” ì´ ë³€í™˜ì„ í†µí•´ ì†ì‹¤ Î¸ì˜ ê¸°ìš¸ê¸°ë¥¼ ì—­ì „íŒŒí•˜ê³  
> BN ë³€í™˜ì˜ ë§¤ê°œ ë³€ìˆ˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.
> ë‹¤ìŒê³¼ ê°™ì€ ì²´ì¸ ê·œì¹™ì„ ì‚¬ìš©í•©ë‹ˆë‹¤(ê°„ë‹¨í™” ì „).

![bn3_math7](../pics/bn3_math7.png)

Thus, BN transform is a differentiable transformation that
introduces normalized activations into the network.

> ë”°ë¼ì„œ BN ë³€í™˜ì€ ë„¤íŠ¸ì›Œí¬ì— ì •ê·œí™”ëœ í™œì„±í™”ë¥¼ ë„ì…í•˜ëŠ” ì°¨ë³„í™” ê°€ëŠ¥í•œ ë³€í™˜ì´ë‹¤.

This  ensures that as the model is training, layers can continue  learning on input distributions that exhibit less internal covariate
shift, thus accelerating the training.

> ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ í›ˆë ¨ ì¤‘ì¸ ë™ì•ˆ ê³„ì¸µì€ ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì„ ë‚˜íƒ€ë‚´ëŠ” ì…ë ¥ ë¶„í¬ë¥¼ ê³„ì† 
> í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í›ˆë ¨ì„ ê°€ì†í™”í•  ìˆ˜ ìˆë‹¤.

Furthermore,  the learned affine transform applied to these normalized
activations allows the BN transform to represent the identity
transformation and preserves the network capacity.

> ë˜í•œ, ì´ëŸ¬í•œ í‘œì¤€í™”ëœ í™œì„±í™”ì— ì ìš©ëœ í•™ìŠµëœ ì•„í•€ ë³€í™˜ì„ í†µí•´ BN ë³€í™˜ì´ ì •ì²´ì„± ë³€í™˜ì„ 
> ë‚˜íƒ€ë‚´ê³  ë„¤íŠ¸ì›Œí¬ ìš©ëŸ‰ì„ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤.

### 3.1. Training and Inference with Batch-Normalized Networks
> BN ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•œ í•™ìŠµ ë° ì¶”ë¡ 

To Batch-Normalize a network, we specify a subset of activations 
and insert the BN transform for each of them, according to __Alg. 1__

> ë„¤íŠ¸ì›Œí¬ë¥¼ ì¼ê´„ ì •ê·œí™”í•˜ê¸° ìœ„í•´ í™œì„±í™”ì˜ í•˜ìœ„ ì§‘í•©ì„ ì§€ì •í•˜ê³  
> ê° í™œì„±í™”ì— ëŒ€í•œ BN ë³€í™˜ì„ ì‚½ì…í•©ë‹ˆë‹¤.

Any layer that previously received
x as the input, now receives BN(x).

> ì–´ë– í•œ ë ˆì´ì–´ëŠ” ì´ì „ì˜ ì¸í’‹xë¥¼ ë°›ê³ , í˜„ì¬ëŠ” BN(x)ë¥¼ ë°›ëŠ”ë‹¤.

A model employing
Batch Normalization can be trained using batch gradient
descent, or Stochastic Gradient Descent with a mini-batch
size m > 1, or with any of its variants such as Adagrad

> ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ ê²½ì‚¬ì  ë°°ì¹˜ í•˜ê°•ë²• ë˜ëŠ”
> ìµœì†Œ ë°°ì¹˜ í¬ê¸° m > 1ë¡œ í™•ë¥ ì  ë°°ì¹˜í•˜ê°•ë²•ì„ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤.

The normalization of activations that
depends on the mini-batch allows efficient training, but is
neither necessary nor desirable during inference;

> ë¯¸ë‹ˆ ë°°ì¹˜ì— ì˜ì¡´í•˜ëŠ” í™œì„±í™”ì˜ ì •ê·œí™”ëŠ” íš¨ìœ¨ì ì¸ í›ˆë ¨ì„ ê°€ëŠ¥ì¼€ í•˜ì§€ë§Œ 
> ì¶”ë¡  ì¤‘ì—ëŠ” ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ë°”ëŒì§í•˜ì§€ ì•Šë‹¤.

we want  the output to depend only on the input, deterministically.
For this, once the network has been trained, we use the  normalization
> ìš°ë¦¬ëŠ” ê²°ì •ì ìœ¼ë¡œ ì¶œë ¥ì´ ì…ë ¥ì—ë§Œ ì˜ì¡´í•˜ê¸°ë¥¼ ì›í•œë‹¤. 
> ì´ë¥¼ ìœ„í•´ ë„¤íŠ¸ì›Œí¬ê°€ í›ˆë ¨ë˜ë©´ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

![bn3_math8](../pics/bn3_math8.png)

using the population, rather than mini-batch, statistics.
Neglecting ğœ– , these normalized activations have the same
mean 0 and variance 1 as during training

> ìµœì†Œ í†µê³„ëŸ‰ ëŒ€ì‹  ëª¨ì§‘ë‹¨ì„ ì‚¬ìš©í•œë‹¤.
> ğœ– ë¥¼ ë¬´ì‹œí•˜ê³ , ì´ëŸ¬í•œ ì •ê·œí™”ëœ í™œì„±í™”ëŠ” í›ˆë ¨ ì¤‘ê³¼ ë™ì¼í•œ í‰ê·  0 ê³¼ ë¶„ì‚° 1ì„ ê°€ì§‘ë‹ˆë‹¤.

We use the unbiased
variance estimate ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]=ğ‘š/(ğ‘šâˆ’1)â‹…ğ¸ğ›½[ğœ2ğ›½], where
the expectation is over trainingmini-batches of size m and ğœ2ğ›½ 
are their sample variances

> ìš°ë¦¬ëŠ” m í¬ê¸°ì˜ ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ í›ˆë ¨í•  ë•Œ ê¸°ëŒ€ì¹˜ê°€ ì´ˆê³¼ë˜ê³  
> ğœ2ğ›½ ê°€ í‘œë³¸ ë¶„ì‚°ì¸ í¸í–¥ë˜ì§€ ì•Šì€ ë¶„ì‚° ì¶”ì •ì¹˜ Var[x]ë¥¼ ì‚¬ìš©í•œë‹¤.

Using moving averages instead,
we can track the accuracy of a model as it trains.

> ëŒ€ì‹  ì´ë™ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ í›ˆë ¨ë˜ëŠ” ë™ì•ˆ ì •í™•ì„±ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Since the means and variances are fixed during inference,
the normalization is simply a linear transform applied to each activation
> ì¶”ë¡ í•˜ëŠ” ë™ì•ˆ í‰ê· ê³¼ ë¶„ì‚°ì´ ê³ ì •ë˜ë¯€ë¡œ ì •ê·œí™”ëŠ” ë‹¨ìˆœíˆ ê° í™œì„±í™”ì— ì ìš©ë˜ëŠ” ì„ í˜• ë³€í™˜ì…ë‹ˆë‹¤.

It may further be composed with the scaling by ğ›¾ and shift by Î², 
to yield a single linear transform that replaces BN(x).
> ì´ê²ƒì€ BN(x)ì„ ëŒ€ì²´í•˜ëŠ” ë‹¨ì¼ ì„ í˜• ë³€í™˜ì„ ìƒì„±í•˜ê¸° ìœ„í•´ 
> ğ›¾ì— ì˜í•œ ìŠ¤ì¼€ì¼ë§ê³¼ Î²ì— ì˜í•œ ì‹œí”„íŠ¸ë¡œ ì¶”ê°€ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆë‹¤.

__Algorithm 2__ summarizes the procedure
for training batch-normalized networks.
> Algorithm 2 ëŠ” ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ì ˆì°¨ë¥¼ ìš”ì•½í•œë‹¤.

![bn_algorithm2.png](../pics/bn_algorithm2.png)

### 3.2 Batch-Normalized Convolutional Networks
> CNNì—ì„œì˜ ë°°ì¹˜ ì •ê·œí™”

Batch Normalization can be applied to any set of activations
in the network.
> ë°°ì¹˜ ì •ê·œí™”ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  í™œì„±í™” ì§‘í•©ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Here, we focus on transforms  that consist of an affine 
transformation followed by an element-wise nonlinearity:
> ì—¬ê¸°ì„œëŠ” ì•„í•€ ë³€í™˜ê³¼ ìš”ì†Œë³„ ë¹„ì„ í˜•ì„±ìœ¼ë¡œ êµ¬ì„±ëœ ë³€í™˜ì— ì´ˆì ì„ ë§ì¶˜ë‹¤.

![bn3_math9](../pics/bn3_math9.png)

where W and b are learned parameters of the model, and
g(â‹…) is the nonlinearity such as sigmoid or ReLU
> ì—¬ê¸°ì„œ Wì™€ bëŠ” ëª¨ë¸ì˜ í•™ìŠµëœ ë§¤ê°œë³€ìˆ˜ì´ê³ ,
> g(â‹…)ëŠ” ì‹œê·¸ëª¨ì´ë“œ ë˜ëŠ” ReLUì™€ ê°™ì€ ë¹„ì„ í˜•ì„±ì´ë‹¤.

This formulation covers both fully-connected and convolutional layers.
> ì´ ê³µì‹ì€ ì™„ì „íˆ ì—°ê²°ëœ ë ˆì´ì–´ì™€ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•œë‹¤.

We add the BN transform immediately before the
nonlinearity, by normalizing x = Wu+b.
> ìš°ë¦¬ëŠ” ë¹„ì„ í˜•ì„± ë°”ë¡œ ì•ì— x = Wu+bë¥¼ ì •ê·œí™”í•˜ì—¬ BN ë³€í™˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

We could have  also normalized the layer inputs u, 
but since u is likely  the output of another nonlinearity,
the shape of its distribution is likely to change during training, 
and constraining  its first and second moments would not eliminate
the covariate  shift.

> ë ˆì´ì–´ ì…ë ¥ uë„ ì •ê·œí™”í•  ìˆ˜ ìˆì§€ë§Œ,
> uëŠ” ë‹¤ë¥¸ ë¹„ì„ í˜•ì„±ì˜ ì¶œë ¥ì¼ ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì—,
> í›ˆë ¨ ì¤‘ì— ë¶„í¬ì˜ ëª¨ì–‘ì´ ë³€ê²½ë  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë©°, 
> ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ëª¨ë©˜íŠ¸ë¥¼ ì œí•œí•´ë„ ê³µë³€ëŸ‰ ì´ë™ì´ ì œê±°ë˜ì§€ ì•ŠëŠ”ë‹¤.

In contrast, Wu + b is more likely to have a symmetric, 
non-sparse distribution, that is â€œmore Gaussianâ€ (Hyvarinen & Oja, 2000);
> ë°˜ëŒ€ë¡œ, Wu + bëŠ” ëŒ€ì¹­ì˜ non-sparse ë¶„í¬ë¥¼ ê°€ì§ˆ ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤. : 

normalizing it is likely to
produce activations with a stable distribution.
> ì •ê·œí™”í•˜ë©´ ì•ˆì •ì ì¸ ë¶„í¬ë¡œ í™œì„±í™”ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

Note that, since we normalize Wu+b, the bias b can be
ignored since its effect will be canceled by the subsequent
mean subtraction (the role of the bias is subsumed by Î² in
__Alg. 1__). 

> ì•Œì•„ë‘¬ìš”, ìš°ë¦¬ê°€ ì •ê·œí™” í•˜ë©´Wu+b, ë°”ì´ì–´ìŠ¤ bëŠ” ê·¸ íš¨ê³¼ê°€ 
> í›„ì†ì ì¸ í‰ê·  ì°¨ê° ì— ì˜í•´ ì·¨ì†Œë˜ê¸° ë•Œë¬¸ì— ë¬´ì‹œë  ìˆ˜ ìˆë‹¤.
> (í¸í–¥ì˜ ì—­í• ì€ Î²ì— í¬í•¨ëœë‹¤.)

Thus, z = g(Wu + b) is replaced with

![](../pics/bn3_math10.png)

where the BN transform is applied independently to each
dimension of x = Wu, with a separate pair of learned
parameters Î³(k), Î²(k) per dimension.

> ì—¬ê¸°ì„œ BN ë³€í™˜ì€ x = Wuì˜ ê° ì¹˜ìˆ˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ë©°, 
> í•™ìŠµëœ ë§¤ê°œë³€ìˆ˜ ìŒì€ ì°¨ì› ë‹¹ Î³(k), Î²(k)ì´ë‹¤.

For convolutional layers, we additionally want the normalization
to obey the convolutional property â€“ so that
different elements of the same feature map, at different
locations, are normalized in the same way.

> ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ê²½ìš° ì •ê·œí™”ê°€ ì»¨ë³¼ë£¨ì…˜ ì†ì„±ì„ ë”°ë¥´ê¸°ë¥¼ ì¶”ê°€ë¡œ ì›í•˜ë¯€ë¡œ 
> ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œ ë™ì¼í•œ í”¼ì³ ë§µì˜ ë‹¤ë¥¸ ìš”ì†Œê°€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”ëœë‹¤.

To achieve  this, we jointly normalize all the activations
in a minibatch, over all locations

> ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë“  ìœ„ì¹˜ì— ê±¸ì³ ë¯¸ë‹ˆë°°ì¹˜ì˜ ëª¨ë“  í™œì„±í™”ë¥¼ ê³µë™ìœ¼ë¡œ ì •ê·œí™”í•œë‹¤.
 
In __Alg. 1__, we let B be the set of
all values in a feature map across both the elements of a
mini-batch and spatial locations â€“ so for a mini-batch of
size m and feature maps of size p x q, we use the effective
mini-batch of size mâ€² = |B| = m â‹… p q.

> ìš°ë¦¬ëŠ” Bë¥¼ ë¯¸ë‹ˆ ë°°ì¹˜ì™€ ê³µê°„ ìœ„ì¹˜ ëª¨ë‘ì— ê±¸ì¹œ í˜•ìƒ ë§µì˜ ëª¨ë“  ê°’ ì§‘í•©ì´ ë˜ë„ë¡ í•œë‹¤.
> ë”°ë¼ì„œ í¬ê¸° mì˜ ë¯¸ë‹ˆ ë°°ì¹˜ì™€ í¬ê¸° p x qì˜ í˜•ìƒ ë§µì˜ ê²½ìš°,
> ìš°ë¦¬ëŠ” ë¯¸ë‹ˆë°°ì¹˜ ì‚¬ì´ì¦ˆ  mâ€² = |B| = m â‹… p q ë¥¼ ì‚¬ìš©í•œë‹¤.

We learn a
pair of parameters Î³(k) and Î²(k) per feature map, rather
than per activation.
> í™œì„±í™” ë‹¹ì´ ì•„ë‹ˆë¼ íŠ¹ì§• ë§µë‹¹ ë§¤ê°œ ë³€ìˆ˜ Î¸(k)ì™€ Î²(k) ìŒì„ í•™ìŠµí•œë‹¤.

__Alg. 2__ is modified similarly, so that
during inference the BN transform applies the same linear
transformation to each activation in a given feature map.

> Alg. 2ëŠ” ìœ ì‚¬í•˜ê²Œ ìˆ˜ì •ë˜ì–´ ì¶”ë¡  ì¤‘ì— BN ë³€í™˜ì€ ì£¼ì–´ì§„ í˜•ìƒ ë§µì˜ ê° 
> í™œì„±í™”ì— ë™ì¼í•œ ì„ í˜• ë³€í™˜ì„ ì ìš©í•œë‹¤.


### 3.3 Batch Normalization enables higher learning rates
> ë°°ì¹˜ì •ê·œí™”ëŠ” ë†’ì€ í•™ìŠµìœ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

In traditional deep networks, too-high learning rate may
result in the gradients that explode or vanish, as well as
getting stuck in poor local minima. 
> ì „í†µì ì¸ ë”¥ ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ì§€ì—­ ìµœì†Œí™”ê°€ ì˜ëª»ë˜ì–´
> í­ë°œí•˜ê±°ë‚˜ ì‚¬ë¼ì§€ëŠ” ê·¸ë ˆì´ë””ì–¸íŠ¸ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

Batch Normalization helps address these issues.
> ë°°ì¹˜ì •ê·œí™”ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

By normalizing activations
throughout the network, it prevents small changes
to the parameters from amplifying into larger and suboptimal
changes in activations in gradients;

> ë„¤íŠ¸ì›Œí¬ ì „ì²´ì˜ í™œì„±í™”ë¥¼ ì •ê·œí™”í•¨ìœ¼ë¡œì¨ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì‘ì€ ë³€í™”ê°€ 
> ê·¸ë ˆì´ë””ì–¸íŠ¸ì˜ í™œì„±í™”ì˜ ë” í¬ê³  ì°¨ì„ ì˜ ë³€í™”ë¡œ ì¦í­ë˜ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤.

for instance, it prevents the training 
from getting stuck in the saturated regimes of nonlinearities.
> ì˜ˆë¥¼ ë“¤ì–´, ì´ê²ƒì€ í›ˆë ¨ì´ í¬í™” ìƒíƒœì˜ ë¹„ì„ í˜•ì„±ì— ê°‡íˆëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤.

Batch Normalization also makes training 
more resilient to the parameter scale.
> ë°°ì¹˜ ì •ê·œí™”ë¥¼ í†µí•´ êµìœ¡ì„ ë§¤ê°œ ë³€ìˆ˜ ì²™ë„ì— ë³´ë‹¤ íƒ„ë ¥ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Normally, large learning rates may
increase the scale of layer parameters, which then amplify
the gradient during backpropagation and lead to the model
explosion.
> ì¼ë°˜ì ìœ¼ë¡œ, í° í•™ìŠµ ì†ë„ëŠ” ê³„ì¸µ ë§¤ê°œ ë³€ìˆ˜ì˜ í¬ê¸°ë¥¼ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, 
> ì´ëŠ” ì—­ì „íŒŒ ì¤‘ ê¸°ìš¸ê¸°ë¥¼ ì¦í­ì‹œí‚¤ê³  ëª¨ë¸ í­ë°œë¡œ ì´ì–´ì§„ë‹¤.

However, with Batch Normalization, backpropagation
through a layer is unaffected by the scale of
its parameters. 
> ê·¸ëŸ¬ë‚˜ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë©´ ë„ë©´ì¸µì„ í†µí•œ ì—­ì „ë‹¬ì€ ë§¤ê°œë³€ìˆ˜ì˜ ê·œëª¨ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤

Indeed, for a scalar a,

![](../pics/bn3_math11.png)

and we can show that

![](../pics/bn3_math12.png)

The scale does not affect the layer Jacobian nor, consequently,
the gradient propagation.

> ì´ ì²™ë„ëŠ” ë ˆì´ì–´ ì•¼ì½”ë¹„ì•ˆì—ë„ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šìœ¼ë©° 
> ê²°ê³¼ì ìœ¼ë¡œ ê²½ì‚¬ ì „íŒŒì—ë„ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤.


Moreover, larger  weights lead to smaller gradients, 
and Batch Normalization  will stabilize the parameter growth.
> ë˜í•œ ê°€ì¤‘ì¹˜ê°€ í´ìˆ˜ë¡ ê·¸ë ˆì´ë””ì–¸íŠ¸ê°€ ì‘ì•„ì§€ê³  ë°°ì¹˜ ì •ê·œí™”ê°€ íŒŒë¼ë¯¸í„° ì¦ê°€ë¥¼ ì•ˆì •í™”ì‹œí‚¨ë‹¤.

---
We further conjecture that Batch Normalization may
lead the layer Jacobians to have singular values close to 1,
which is known to be beneficial for training (Saxe et al.,
2013).

> ìš°ë¦¬ëŠ” ë°°ì¹˜ ì •ê·œí™”ê°€ ë ˆì´ì–´ ì•¼ì½”ë¹„ì•ˆë“¤ë¡œ í•˜ì—¬ê¸ˆ í›ˆë ¨ì— ìœ ìµí•œ 1ì— ê°€ê¹Œìš´ 
> singular values ì„ ê°–ë„ë¡ í•  ìˆ˜ ìˆë‹¤ê³  ë” ì¶”ì¸¡í•œë‹¤.

Consider two consecutive layers with normalized inputs, 
and the transformation between these normalized vectors:  ğ‘§Ì‚ =ğ¹(ğ‘¥Ì‚ ) .

> ì •ê·œí™”ëœ ì…ë ¥ì„ ê°€ì§„ ë‘ ê°œì˜ ì—°ì† ë ˆì´ì–´ì™€ ì •ê·œí™”ëœ ë²¡í„° ì‚¬ì´ì˜ ë³€í™˜ì„ ê³ ë ¤í•˜ì.

If we assume that ğ‘¥Ì‚  and ğ‘§Ì‚ are Gaussian
and uncorrelated, and that ğ¹(ğ‘¥Ì‚ ) â‰ˆ Jğ‘¥Ì‚ is a linear transformation
for the given model parameters, then both ğ‘¥Ì‚ and ğ‘§Ì‚ 
have unit covariances

> ë§Œì•½ ğ‘¥Ì‚ ì™€ ğ‘§Ì‚ ê°€ ê°€ìš°ìŠ¤ì´ê³  ìƒê´€ê´€ê³„ê°€ ì—†ìœ¼ë©°, 
> ğ¹(ğ‘¥Ì‚)ê°€ ì£¼ì–´ì§„ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì„ í˜• ë³€í™˜ì´ë¼ê³  ê°€ì •í•œë‹¤ë©´,
> ğ‘¥Ì‚ ì™€ ğ‘§Ì‚ ëŠ” ëª¨ë‘ ë‹¨ìœ„ ê³µë¶„ì‚°ì„ ê°–ëŠ”ë‹¤.

![](../pics/bn3_math13.png)
> ì¦ëª…ê³¼ì •ì´ë©°, ë‹¨ì¼ê°’ Jê°€ 1ì´ë‹¤(?)
> ì´ëŠ” ì—­ì „íŒŒ ì¤‘ì— ê¸°ìš¸ê¸° í¬ê¸°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

In reality, the transformation is
not linear, and the normalized values are not guaranteed to
be Gaussian nor independent, but we nevertheless expect
Batch Normalization to help make gradient propagation
better behaved.

> ì‹¤ì œë¡œëŠ” ë³€í™˜ì´ ì„ í˜•ì ì´ì§€ ì•Šê³  
> ì •ê·œí™”ëœ ê°’ì´ ê°€ìš°ìŠ¤ì´ê±°ë‚˜ ë…ë¦½ì ì´ë¼ê³  ë³´ì¥ë˜ì§€ ì•Šì§€ë§Œ,
> ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë°°ì¹˜ ì •ê·œí™”ê°€ ê·¸ë ˆì´ë””ì–¸íŠ¸ ì „íŒŒë¥¼ 
> ë” ì˜ ì‘ë™ì‹œí‚¤ëŠ” ë° ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.

The precise effect of Batch Normalization on 
gradient propagation remains an area of further study.
> ë°°ì¹˜ ì •ê·œí™”ê°€ ê¸°ìš¸ê¸° ì „íŒŒì— ë¯¸ì¹˜ëŠ” ì •í™•í•œ ì˜í–¥ì€ ì¶”ê°€ ì—°êµ¬ ì˜ì—­ìœ¼ë¡œ ë‚¨ì•„ ìˆë‹¤.

### 3.4. Batch Normalization regularizes the model
> ë°°ì¹˜ì •ê·œí™”ëŠ” ëª¨ë¸ì„ ê·œì œí•œë‹¤.

When training with Batch Normalization, a training example
is seen in conjunction with other examples in the
mini-batch, and the training network no longer producing
deterministic values for a given training example.

> ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ë•Œ, 
> í›ˆë ¨ ì˜ˆì œëŠ” ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ì˜ˆì™€ í•¨ê»˜ ë³´ì—¬ì§€ë©°,
> í›ˆë ¨ ë„¤íŠ¸ì›Œí¬ëŠ” ì£¼ì–´ì§„ í›ˆë ¨ ì˜ˆì— ëŒ€í•œ ê²°ì •ë¡ ì  ê°’ì„ ë” ì´ìƒ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.

In  our experiments, we found this effect to be advantageous
to the generalization of the network.
> ìš°ë¦¬ì˜ ì‹¤í—˜ì—ì„œ, ìš°ë¦¬ëŠ” ì´ íš¨ê³¼ê°€ ë„¤íŠ¸ì›Œí¬ì˜ ì¼ë°˜í™”ì— ìœ ë¦¬í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

Whereas Dropout (Srivastava et al., 2014) is typically used to 
reduce overfitting,  in a batch-normalized network we found 
that it can be either removed or reduced in strength.
> ë“œë¡­ì•„ì›ƒì€ ì¼ë°˜ì ìœ¼ë¡œ ê³¼ì í•©ì„ ì¤„ì´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë°˜ë©´, 
> ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì œê±°í•˜ê±°ë‚˜ ê°•ë„ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

## 4. Experiments
### 4.1 Activations over time
To verify the effects of internal covariate shift on training,
and the ability of Batch Normalization to combat it,
we considered the problem of predicting the digit class on
the MNIST dataset
> ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì´ í—‰ìˆ©ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°°ì¹˜ ì •ê·œí™”ì˜ 
> ëŠ¥ë ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ MNIST ë°ì´í„° ì„¸íŠ¸ì˜ ìˆ«ì í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ ê³ ë ¤í–ˆë‹¤.

We used a very simple network, with a 28x28 binary image as input,
and fully-connected hidden layers with 100 activations each.
> 28x28 ì´ì§„ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë§¤ìš° ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ì™€ 
> ê°ê° 100ê°œì˜ í™œì„±í™”ë¡œ ì™„ì „íˆ ì—°ê²°ëœ ì€ë‹‰ì¸µì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

Each hidden layer computes y = g(Wu+b) with sigmoid  
nonlinearity, and the weights W initialized to small random
Gaussian values. 
> ê° ì€ë‹‰ ê³„ì¸µì€ ì‹œê·¸ëª¨ì´ë“œ ë¹„ì„ í˜•ì„±ì„ ê°–ëŠ” y = g(Wu+b)ì™€ 
> ì‘ì€ ë¬´ì‘ìœ„ ê°€ìš°ìŠ¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ Wë¥¼ ê³„ì‚°í•œë‹¤.

The last hidden layer is followed
by a fully-connected layer with 10 activations (one per
class) and cross-entropy loss
> ë§ˆì§€ë§‰ ì€ë‹‰ ë ˆì´ì–´ëŠ” 10ê°œì˜ í™œì„±í™”(í´ë˜ìŠ¤ë‹¹ 1ê°œ)ì™€ 
> êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ë¡œ ì™„ì „íˆ ì—°ê²°ëœ ë ˆì´ì–´ê°€ ë’¤ë”°ë¥¸ë‹¤.

We trained the network for
50000 steps, with 60 examples per mini-batch. 
We added Batch Normalization to each hidden layer 
of the network, as in Sec. 3.1
> ë¯¸ë‹ˆ ë°°ì¹˜ë‹¹ 60ê°œì˜ ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ì—¬ 50000ë‹¨ê³„ì— ëŒ€í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í–ˆë‹¤.
> 3.1ì ˆê³¼ ê°™ì´ ë„¤íŠ¸ì›Œí¬ì˜ ìˆ¨ê²¨ì§„ ê° ê³„ì¸µì— ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì¶”ê°€í•˜ì˜€ë‹¤.

We were interested in the comparison between
the baseline and batch-normalized networks, rather
than achieving the state of the art performance on MNIST
(which the described architecture does not).
> ìš°ë¦¬ëŠ” MNISTì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥(ì„¤ëª…ëœ ì•„í‚¤í…ì²˜ê°€ ë‹¬ì„±í•˜ì§€ ëª»í•˜ëŠ”)ì„ ë‹¬ì„±í•˜ê¸°ë³´ë‹¤ëŠ”
> ê¸°ì¤€ ë„¤íŠ¸ì›Œí¬ì™€ ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ ê°„ì˜ ë¹„êµì— ê´€ì‹¬ì´ ìˆì—ˆë‹¤.

![](../pics/bn4_1.png)

Figure 1(a) shows the fraction of correct predictions
by the two networks on held-out test data, as training
progresses
> ê·¸ë¦¼ 1(a)ëŠ” í™€ë“œ-ì•„ì›ƒ í…ŒìŠ¤íŠ¸ì—ì„œ ë‘ ë„¤íŠ¸ì›Œí¬ì— ì˜í•œ ì •í™•í•œ 
> ì˜ˆì¸¡ ë¹„ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. êµìœ¡ì´ ì§„í–‰ë¨ì— ë”°ë¼ ë°ì´í„°

The batch-normalized network enjoys the
higher test accuracy. To investigate why, we studied inputs
to the sigmoid, in the original network N and batch-normalized
network  BN (Alg. 2) over the course of training.


> ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ëŠ” í…ŒìŠ¤íŠ¸ ì •í™•ë„ê°€ ë†’ìŠµë‹ˆë‹¤. 
> ì´ìœ ë¥¼ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ í›ˆë ¨ ê³¼ì •ì— ê±¸ì³ ì›ë˜ ë„¤íŠ¸ì›Œí¬ Nê³¼
> ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ BN(Alg. 2)ì—ì„œ ì‹œê·¸ëª¨ì´ë“œì— ëŒ€í•œ ì…ë ¥ì„ ì—°êµ¬í–ˆë‹¤.

 
In Fig. 1(b,c) we show, for one typical activation from  the 
last hidden layer of each network, how its distribution evolves
> ê·¸ë¦¼ 1(b,c)ì—ì„œëŠ” ê° ë„¤íŠ¸ì›Œí¬ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ê³„ì¸µì—ì„œ í•˜ë‚˜ì˜
> ì¼ë°˜ì ì¸ í™œì„±í™”ì— ëŒ€í•´ ë¶„í¬ê°€ ì–´ë–»ê²Œ ì§„í™”í•˜ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤.

The distributions in the original network
change significantly over time, both in their mean and
the variance, which complicates the training of the subsequent
layers
> ì›ë˜ ë„¤íŠ¸ì›Œí¬ì˜ ë¶„í¬ëŠ” í‰ê· ê³¼ ë¶„ì‚° ëª¨ë‘ì—ì„œ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ í¬ê²Œ ë³€í™”í•˜ë©°,
> ì´ëŠ” í›„ì† ê³„ì¸µì˜ í›ˆë ¨ì„ ë³µì¡í•˜ê²Œ í•œë‹¤.

In contrast, the distributions in the batchnormalized
network are much more stable as training progresses,
which aids the training.
> ëŒ€ì¡°ì ìœ¼ë¡œ, ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ì˜ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í›¨ì”¬ ë” ì•ˆì •ì ì´ë‹¤.
> í›ˆë ¨ì˜ ì§„í–‰, í›ˆë ¨ì— ë„ì›€ì´ ëœë‹¤.

### 4.2. ImageNet classification
We applied Batch Normalization to a new variant of the
Inception network  trained on the ImageNet classification task.
> ìš°ë¦¬ëŠ” ImageNet ë¶„ë¥˜ ì‘ì—…ì— ëŒ€í•´ í›ˆë ¨ëœ ìƒˆë¡œìš´ ë³€í˜•
> ì¸ì…‰ì…˜ ë„¤íŠ¸ì›Œí¬ì— ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í–ˆë‹¤.

The network has a large number of convolutional and
pooling layers, with a softmax layer to predict the image
class, out of 1000 possibilities.
> ë„¤íŠ¸ì›Œí¬ì—ëŠ” ì´ë¯¸ì§€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ ë ˆì´ì–´ì™€ í•¨ê»˜
> 1000ê°œì˜ ê°€ëŠ¥ì„± ì¤‘ ë§ì€ ìˆ˜ì˜ ì»¨ë³¼ë£¨ì…˜ ë° í’€ë§ ë ˆì´ì–´ê°€ ìˆë‹¤.

Convolutional layers use ReLU as the nonlinearity
> ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ëŠ” ë¹„ì„ í˜•ì„±ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

The main difference to the network
described in is that the 5 * 5
convolutional layers are replaced by two consecutive layers
of 3 * 3 convolutions with up to 128 filters
> ì•ì—ì„œ ì„¤ëª…í•œ ë„¤íŠ¸ì›Œí¬ì™€ì˜ ì£¼ìš” ì°¨ì´ì ì€ 5 * 5 ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ê°€ ìµœëŒ€ 
> 128ê°œì˜ í•„í„°ê°€ ìˆëŠ” 3 * 3 ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ë‘ ê°œì˜ ì—°ì†
> ë ˆì´ì–´ë¡œ ëŒ€ì²´ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

The network contains 13.6 *10^6 parameters, and, other than 
the top softmax layer, has no fully-connected layers.
More details are given the Appendix.
> ë„¤íŠ¸ì›Œí¬ì—ëŠ” 13.6 *10^6 íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ë˜ë©°, ìƒë‹¨ ì†Œí”„íŠ¸ë§¥ìŠ¤
> ë ˆì´ì–´ ì™¸ì— ì™„ì „íˆ ì—°ê²°ëœ ë ˆì´ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.
> ìì„¸í•œ ë‚´ìš©ì€ ë¶€ë¡ì„ ì°¸ì¡°í•˜ì„¸ìš”.

We refer to this model
as Inception in the rest of the text. The model was trained
using a version of Stochastic Gradient Descent with momentum(
Sutskever et al., 2013), using themini-batch size
of 32.

> ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì„ ë³¸ë¬¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì—ì„œ ì¸ì…‰ì…˜ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
> ëª¨ë¸ì€ 32ì˜ ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë©˜í…€ì´ ìˆëŠ” í™•ë¥ ì 
> ê²½ì‚¬ ê°•í•˜ ë²„ì „ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë˜ì—ˆë‹¤.

The trainingwas performed using a large-scale, distributed
architecture (similar to (Dean et al., 2012)). All
networks are evaluated as training progresses by computing
the validation accuracy @1, i.e. the probability of
predicting the correct label out of 1000 possibilities, on
a held-out set, using a single crop per image.
> í›ˆë ¨ì€ ëŒ€ê·œëª¨ ë¶„ì‚° ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë˜ì—ˆë‹¤(Dean et al., 2012). 
> ëª¨ë“  ë„¤íŠ¸ì›Œí¬ëŠ” ê²€ì¦ ì •í™•ë„ @1, ì¦‰ ì´ë¯¸ì§€ë‹¹ ë‹¨ì¼ í¬ë¡­ì„ ì‚¬ìš©í•˜ì—¬ í™€ë“œì•„ì›ƒ 
> ì„¸íŠ¸ì—ì„œ 1000ê°œì˜ ê°€ëŠ¥ì„± ì¤‘ ì˜¬ë°”ë¥¸ ë ˆì´ë¸”ì„ 
> ì˜ˆì¸¡í•  í™•ë¥ ì„ ê³„ì‚°í•¨ìœ¼ë¡œì¨ êµìœ¡ ì§„í–‰ë¥ ë¡œ í‰ê°€ëœë‹¤.

In our experiments, we evaluated several modifications
of Inception with Batch Normalization. In all cases, Batch
Normalization was applied to the input of each nonlinearity,
in a convolutional way, as described in section 3.2,
while keeping the rest of the architecture constant.
> ì‹¤í—˜ì—ì„œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ ì¸ì…‰ì…˜ì˜ ëª‡ ê°€ì§€ ìˆ˜ì • ì‚¬í•­ì„ í‰ê°€í–ˆë‹¤.
> ëª¨ë“  ê²½ìš°ì—, ë°°ì¹˜ ì •ê·œí™”ëŠ” ì•„í‚¤í…ì²˜ì˜ ë‚˜ë¨¸ì§€ë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€í•˜ë©´ì„œ, 
> ì„¹ì…˜ 3.2ì—ì„œ ì„¤ëª…ëœ ëŒ€ë¡œ, ì»¨ë³¼ë£¨ì…˜ ë°©ì‹ìœ¼ë¡œ, ê° ë¹„ì„ í˜•ì„±ì˜ ì…ë ¥ì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

#### 4.2.1 Accelerating BN Networks
Simply adding Batch Normalization to a network does not
take full advantage of our method. To do so, we further
changed the network and its training parameters, as follows:
> ë‹¨ìˆœíˆ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ë„¤íŠ¸ì›Œí¬ì— ì¶”ê°€í•˜ëŠ” ê²ƒì€ ìš°ë¦¬ì˜ ë°©ë²•ì„ ìµœëŒ€í•œ í™œìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
> ì´ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ë„¤íŠ¸ì›Œí¬ì™€ êµìœ¡ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì¶”ê°€ë¡œ ë³€ê²½í–ˆë‹¤.

Increase learning rate. In a batch-normalized model,
we have been able to achieve a training speedup from
higher learning rates, with no ill side effects (Sec. 3.3).
> í•™ìŠµë¥ ì„ ë†’ì…ë‹ˆë‹¤. ë°°ì¹˜ ì •ê·œí™” ëª¨ë¸ì—ì„œ, ìš°ë¦¬ëŠ” ë¶€ì‘ìš© 
> ì—†ì´ ë” ë†’ì€ í•™ìŠµ ì†ë„ì—ì„œ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆì—ˆë‹¤(3.3í•­).

Remove Dropout. As described in Sec. 3.4, Batch Normalization
fulfills some of the same goals as Dropout. Removing
Dropout from Modified BN-Inception speeds up
training, without increasing overfitting.
> ë“œë¡­ì•„ì›ƒì„ ì œê±°í•©ë‹ˆë‹¤. 3.4í•­ì—ì„œ ì„¤ëª…í•œ ë°”ì™€ ê°™ì´ ë°°ì¹˜ ì •ê·œí™”ëŠ” ë“œë¡­ì•„ì›ƒê³¼ 
> ë™ì¼í•œ ëª©í‘œë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ìˆ˜ì •ëœ BN-ì¸ì…‰ì…˜ì—ì„œ ë“œë¡­ì•„ì›ƒì„ ì œê±°í•˜ë©´ 
> ê³¼ì í•©ì´ ì¦ê°€í•˜ì§€ ì•Šê³  êµìœ¡ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤.

Reduce the L2 weight regularization. While in Inception
an L2 loss on the model parameters controls overfitting,
in Modified BN-Inception the weight of this loss is
reduced by a factor of 5. We find that this improves the
accuracy on the held-out validation data.
> L2 ê°€ì¤‘ì¹˜ ê·œì œë¥¼ ì¤„ì…ë‹ˆë‹¤. Inceptionì—ì„œ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì˜ L2 ì†ì‹¤ì´
> ê³¼ì í•©ì„ ì œì–´í•˜ëŠ” ë°˜ë©´, Modified BN-Inceptionì—ì„œëŠ” 
> ì´ ì†ì‹¤ì˜ ë¬´ê²Œê°€ 5ë°°ë§Œí¼ ê°ì†Œí•œë‹¤. ìš°ë¦¬ëŠ” ì´ê²ƒì´ ë³´ë¥˜ëœ ê²€ì¦ ë°ì´í„°ì˜ 
> ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

Accelerate the learning rate decay. In training Inception,
learning rate was decayed exponentially. Because
our network trains faster than Inception, we lower the
learning rate 6 times faster.
> í•™ìŠµ ì†ë„ ì €í•˜ë¥¼ ê°€ì†í™”í•©ë‹ˆë‹¤. ì¸ì…‰ì…˜ í›ˆë ¨ì—ì„œ í•™ìŠµ ì†ë„ëŠ” ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ 
> ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ ë„¤íŠ¸ì›Œí¬ëŠ” ì¸ì…‰ì…˜ë³´ë‹¤ ë” ë¹¨ë¦¬ í›ˆë ¨í•˜ê¸° ë•Œë¬¸ì—, 
> ìš°ë¦¬ëŠ” í•™ìŠµ ì†ë„ë¥¼ 6ë°° ë” ë¹ ë¥´ê²Œ ë‚®ì¶¥ë‹ˆë‹¤.

Remove Local Response Normalization While Inception
and other networks (Srivastava et al., 2014) benefit
from it, we found that with Batch Normalization it is not
necessary.
> ë¡œì»¬ ì‘ë‹µ ì •ê·œí™” ì œê±° ì¸ì…‰ì…˜ ë° ê¸°íƒ€ ë„¤íŠ¸ì›Œí¬(Srivastava ë“±, 2014)ê°€ 
> ì´ì ì„ ì–»ëŠ” ë™ì•ˆ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.

Shuffle training examples more thoroughly. We enabled
within-shard shuffling of the training data, which prevents
the same examples from always appearing in a mini-batch
together. 
> êµìœ¡ ì˜ˆì œë¥¼ ë” ì² ì €íˆ ì„ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ ìƒ¤ë“œ 
> ë‚´ í˜¼í•©ì„ í™œì„±í™”í•˜ì—¬ ë™ì¼í•œ ì˜ˆê°€ í•­ìƒ ë¯¸ë‹ˆ ë°°ì¹˜ì— í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë°©ì§€í–ˆë‹¤.


This led to about 1% improvements in the validation
accuracy, which is consistent with the view of
Batch Normalization as a regularizer (Sec. 3.4): the randomization
inherent in our method should be most beneficial
when it affects an example differently each time it is
seen.
> ì´ëŠ” ê²€ì¦ ì •í™•ë„ê°€ ì•½ 1% í–¥ìƒìœ¼ë¡œ ì´ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ë°°ì¹˜ ì •ê·œí™”(3.4í•­)ì˜ 
> ê´€ì ê³¼ ì¼ì¹˜í•œë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì— ë‚´ì¬ëœ ë¬´ì‘ìœ„í™”ëŠ” ë³´ì¼ ë•Œë§ˆë‹¤ ì˜ˆì— ë‹¤ë¥¸ 
> ì˜í–¥ì„ ë¯¸ì¹  ë•Œ ê°€ì¥ ìœ ë¦¬í•´ì•¼ í•œë‹¤.

Reduce the photometric distortions. Because batchnormalized
networks train faster and observe each training
example fewer times, we let the trainer focus on more
â€œrealâ€ images by distorting them less.
> ì¸¡ê´‘í•™ì  ì™œê³¡ì„ ì¤„ì…ë‹ˆë‹¤. ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ëŠ” ë” ë¹ ë¥´ê²Œ í›ˆë ¨í•˜ê³  
> ê° í›ˆë ¨ ì˜ˆì œë¥¼ ë” ì ê²Œ ê´€ì°°í•˜ê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” íŠ¸ë ˆì´ë„ˆê°€
> ë” ì ì€ ì™œê³¡ì„ í†µí•´ ë” ë§ì€ "ì‹¤ì œ" ì´ë¯¸ì§€ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

![](../pics/bn4_2.png)

#### 4.2.2 Single-Network Classification

We evaluated the following networks, all trained on the
LSVRC2012 training data, and tested on the validation data:
> ìš°ë¦¬ëŠ” LSVRC2012 í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ í›ˆë ¨ëœ ë‹¤ìŒ ë„¤íŠ¸ì›Œí¬ë¥¼ í‰ê°€í•˜ê³  
> ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸í–ˆë‹¤.

Inception: the network described at the beginning of
Section 4.2, trained with the initial learning rate of 0.0015.
BN-Baseline: Same as Inception with Batch Normalization
before each nonlinearity.
> ì¸ì…‰ì…˜: ì„¹ì…˜ 4.2ì˜ ì‹œì‘ ë¶€ë¶„ì— ê¸°ìˆ ëœ ë„¤íŠ¸ì›Œí¬, 0.0015ì˜ ì´ˆê¸° í•™ìŠµë¥ ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤.
> BN-ê¸°ì¤€: ê° ë¹„ì„ í˜•ì„± ì´ì „ì˜ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ ì¸ì…‰ì…˜ê³¼ ë™ì¼í•©ë‹ˆë‹¤.

BN-x5: Inception with Batch Normalization and the
modifications in Sec. 4.2.1. The initial learning rate was
increased by a factor of 5, to 0.0075. The same learning
rate increase with original Inception caused the model parameters
to reach machine infinity.
> BN-x5: ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ ì¸ì…‰ì…˜ ë° 4.2.1í•­ì˜ ìˆ˜ì •ì‚¬í•­.
> ì´ˆê¸° í•™ìŠµë¥ ì€ 0.0075ë¡œ 5ë°° ì¦ê°€í–ˆë‹¤. ì›ë˜ì˜ ì¸ì…‰ì…˜ê³¼ ë™ì¼í•œ í•™ìŠµ ì†ë„ ì¦ê°€ëŠ”
> ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ê°€ ê¸°ê³„ ë¬´í•œëŒ€ì— ë„ë‹¬í•˜ë„ë¡ ì•¼ê¸°í–ˆë‹¤.

BN-x30: Like BN-x5, but with the initial learning rate
0.045 (30 times that of Inception).
> BN-x30: BN-x5ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ì´ˆê¸° í•™ìŠµë¥ ì€ 0.045(ì¸ì…‰ì…˜ì˜ 30ë°°)ì…ë‹ˆë‹¤.

BN-x5-Sigmoid: Like BN-x5, but with sigmoid nonlinearity
g(t) = $\frac{1}{1+exp(âˆ’x)}$ instead of ReLU. We also attempted
to train the original Inception with sigmoid, but
the model remained at the accuracy equivalent to chance.

> BN-x5-ì‹œê·¸ëª¨ì´ë“œ: BN-x5ì²˜ëŸ¼, í•˜ì§€ë§Œ ReLU ëŒ€ì‹  ì‹œê·¸ëª¨ì´ë“œ ë¹„ì„ í˜•ì„± 
> g(t) = \frac{1}{1+expectx)}$ì¸ ê²½ìš°. ìš°ë¦¬ëŠ” ì‹œê·¸ëª¨ì´ë“œë¡œ ì›ë˜ 
> ì¸ì…‰ì…˜ì„ í›ˆë ¨ì‹œí‚¤ë ¤ í–ˆì§€ë§Œ ëª¨ë¸ì€ ìš°ì—°ì— ìƒì‘í•˜ëŠ” ì •í™•ë„ë¡œ ìœ ì§€ë˜ì—ˆë‹¤.

In Figure 2, we show the validation accuracy of the
networks, as a function of the number of training steps.
Inception reached the accuracy of 72.2% after 31 â€€ 106
training steps. The Figure 3 shows, for each network,
the number of training steps required to reach the same
72.2% accuracy, as well as the maximum validation accuracy
reached by the network and the number of steps to
reach it.
> ê·¸ë¦¼ 2ì—ì„œëŠ” í›ˆë ¨ ë‹¨ê³„ ìˆ˜ì˜ í•¨ìˆ˜ë¡œì„œ ë„¤íŠ¸ì›Œí¬ì˜ ê²€ì¦ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤.
> ì¸ì…‰ì…˜ì€ 31*10^6 í›ˆë ¨ ë‹¨ê³„ í›„ 72.2%ì˜ ì •í™•ë„ì— ë„ë‹¬í–ˆë‹¤.
> ê·¸ë¦¼ 3ì€ ê° ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ ë™ì¼í•œ 72.2%ì˜ ì •í™•ë„ì— ë„ë‹¬í•˜ëŠ” ë° í•„ìš”í•œ
> í›ˆë ¨ ë‹¨ê³„ ìˆ˜ì™€ ë„¤íŠ¸ì›Œí¬ê°€ ë„ë‹¬í•˜ëŠ” ìµœëŒ€ ìœ íš¨ì„± ê²€ì‚¬ ì •í™•ë„ ë° ë„ë‹¬ ë‹¨ê³„ ìˆ˜ë¥¼ ë³´ì—¬ì¤€ë‹¤.

By only using Batch Normalization (BN-Baseline), we
match the accuracy of Inception in less than half the number
of training steps. By applying the modifications in
Sec. 4.2.1, we significantly increase the training speed of
the network. 
> BN-Baseline(Batch Normalization)ë§Œ ì‚¬ìš©í•˜ì—¬ êµìœ¡ ë‹¨ê³„ ìˆ˜ì˜ ì ˆë°˜ ë¯¸ë§Œìœ¼ë¡œ
> Inceptionì˜ ì •í™•ë„ì™€ ì¼ì¹˜í•œë‹¤.
> 4.2.1í•­ì˜ ìˆ˜ì • ì‚¬í•­ì„ ì ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ ì†ë„ë¥¼ í¬ê²Œ ë†’ì¸ë‹¤.

BN-x5 needs 14 times fewer steps than Inception
to reach the 72.2% accuracy. Interestingly, increasing
the learning rate further (BN-x30) causes the
model to train somewhat slower initially, but allows it to
reach a higher final accuracy. It reaches 74.8%after 6*10^6
steps, i.e. 5 times fewer steps than required by Inception
to reach 72.2%.
> BN-x5ëŠ” 72.2%ì˜ ì •í™•ë„ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ ì¸ì…‰ì…˜ë³´ë‹¤ 14ë°° ì ì€ ë‹¨ê³„ê°€ í•„ìš”í•˜ë‹¤. 
> í¥ë¯¸ë¡­ê²Œë„ í•™ìŠµ ì†ë„ë¥¼ ë” ë†’ì´ë©´(BN-x30) ëª¨ë¸ì´ ì²˜ìŒì—ëŠ” ë‹¤ì†Œ ëŠë¦¬ê²Œ í›ˆë ¨ë˜ì§€ë§Œ 
> ë” ë†’ì€ ìµœì¢… ì •í™•ë„ì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤. 6*10^6 ë‹¨ê³„ í›„ 74.8%ì— ë„ë‹¬í•œë‹¤.
> ì¦‰, 72.2%ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ ì¸ì…‰ì…˜ì´ ìš”êµ¬í•˜ëŠ” ë‹¨ê³„ë³´ë‹¤ 5ë°° ì ì€ ë‹¨ê³„ì´ë‹¤.

We also verified that the reduction in internal covariate
shift allows deep networks with Batch Normalization
to be trained when sigmoid is used as the nonlinearity,
despite the well-known difficulty of training such networks.
Indeed, BN-x5-Sigmoid achieves the accuracy of 69.8%.
> ë˜í•œ ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ì˜ ê°ì†ŒëŠ” ê·¸ëŸ¬í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í•˜ëŠ” ë° ì˜ ì•Œë ¤ì§„ ì–´ë ¤ì›€ì—ë„ 
> ë¶ˆêµ¬í•˜ê³  Sigmoidë¥¼ ë¹„ì„ í˜•ì„±ìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ í†µí•´ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ë¥¼ 
> í›ˆë ¨í•  ìˆ˜ ìˆìŒì„ ê²€ì¦í–ˆë‹¤.
> ì‹¤ì œë¡œ BN-x5-ì‹œê·¸ëª¨ì´ë“œëŠ” 69.8%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤.

Without Batch Normalization, Inception with sigmoid
never achieves better than 1/1000 accuracy.
> ë°°ì¹˜ ì •ê·œí™” ì—†ì´ëŠ” ì‹œê·¸ëª¨ì´ë“œê°€ ìˆëŠ” ì¸ì…‰ì…˜ì€ 1000ë¶„ì˜ 1 ì´ìƒì˜ 
> ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

![](../pics/bn4_3.png)

#### 4.2.3 Ensemble Classification

The current reported best results on the ImageNet Large
Scale Visual Recognition Competition are reached by the
Deep Image ensemble of traditional models (Wu et al.,
2015) and the ensemble model of (He et al., 2015). The
latter reports the top-5 error of 4.94%, as evaluated by the
ILSVRC server. Here we report a top-5 validation error of
4.9%, and test error of 4.82% (according to the ILSVRC
server). This improves upon the previous best result, and
exceeds the estimated accuracy of human raters according
to (Russakovsky et al., 2014).

> ImageNet ëŒ€ê·œëª¨ ì‹œê°ì  ì¸ì‹ ëŒ€íšŒì—ì„œ í˜„ì¬ ë³´ê³ ëœ 
> ìµœê³  ê²°ê³¼ëŠ” ê¸°ì¡´ ëª¨ë¸ì˜ ë”¥ ì´ë¯¸ì§€ ì•™ìƒë¸”ì— ì˜í•´ ë‹¬ì„±ë˜ì—ˆë‹¤(Wu ë“±).
> í›„ìëŠ” ILSVRC ì„œë²„ê°€ í‰ê°€í•œ ìƒìœ„ 5ê°œ ì˜¤ë¥˜ë¥¼ 4.94%ë¡œ ë³´ê³ í•œë‹¤.
> ì—¬ê¸°ì„œëŠ” ìƒìœ„ 5ê°œ ê²€ì¦ ì˜¤ë¥˜ 4.9%ì™€ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ 4.82%ë¥¼ ë³´ê³ í•œë‹¤(ILSVRC ì„œë²„ì— ë”°ë¥´ë©´).
> ì´ê²ƒì€ ì´ì „ì˜ ìµœê³  ê²°ê³¼ë¥¼ ê°œì„ í•˜ê³  (Russakovsky ë“±, 2014)ì— ë”°ë¼
> ì¸ê°„ í‰ê°€ìì˜ ì¶”ì • ì •í™•ë„ë¥¼ ì´ˆê³¼í•œë‹¤

Each network achieved its maximum
accuracy after about 6 * 10^6 training steps. The ensemble
prediction was based on the arithmetic average of class
probabilities predicted by the constituent networks. The
details of ensemble and multicrop inference are similar to
(Szegedy et al., 2014).
> ê° ë„¤íŠ¸ì›Œí¬ëŠ” ì•½ 6 * 10^6 í›ˆë ¨ ë‹¨ê³„ë¥¼ ê±°ì¹œ í›„ ìµœëŒ€ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤. 
> ì•™ìƒë¸” ì˜ˆì¸¡ì€ êµ¬ì„± ë„¤íŠ¸ì›Œí¬ê°€ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ í™•ë¥ ì˜ ì‚°ìˆ  í‰ê· ì„ ê¸°ë°˜ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤. 
> ì•™ìƒë¸”ê³¼ ë©€í‹° í¬ë¡­ ì¶”ë¡ ì˜ ì„¸ë¶€ ì‚¬í•­ì€ (Szegedy ë“±, 2014)ì™€ ìœ ì‚¬í•˜ë‹¤.

We demonstrate in Fig. 4 that batch normalization allows
us to set new state-of-the-art by a healthy margin on
the ImageNet classification challenge benchmarks.
> ìš°ë¦¬ëŠ” ê·¸ë¦¼ 4ì—ì„œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ í†µí•´ ImageNet ë¶„ë¥˜ ì±Œë¦°ì§€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ 
> ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ê±´ê°•í•œ ì°¨ì´ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

---
## 5. Conclusion
We have presented a novel mechanism for dramatically
accelerating the training of deep networks. It is based on
the premise that covariate shift, which is known to complicate
the training of machine learning systems, also applies to 
sub-networks and layers, and removing it from
internal activations of the network may aid in training.
> ìš°ë¦¬ëŠ” ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ì„ íšê¸°ì ìœ¼ë¡œ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì‹œí–ˆë‹¤.
> ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ í›ˆë ¨ì„ ë³µì¡í•˜ê²Œ í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ ê³µë³€ëŸ‰ ì‹œí”„íŠ¸ê°€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ì™€
> ê³„ì¸µì—ë„ ì ìš©ëœë‹¤ëŠ” ì „ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ í™œì„±í™”ì—ì„œ ì´ë¥¼ ì œê±°í•˜ë©´
> í›ˆë ¨ì— ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.

Our proposed method draws its power from normalizing
activations, and from incorporating this normalization in
the network architecture itself. This ensures that the normalization
is appropriately handled by any optimization
method that is being used to train the network
> ìš°ë¦¬ê°€ ì œì•ˆí•œ ë°©ë²•ì€ í™œì„±í™”ë¥¼ ì •ê·œí™”í•˜ê³  ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ìì²´ì—
> ì´ ì •ê·œí™”ë¥¼ í†µí•©í•˜ëŠ” ê²ƒì—ì„œ í˜ì„ ì–»ëŠ”ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë„¤íŠ¸ì›Œí¬ë¥¼ êµìœ¡í•˜ëŠ” ë°
> ì‚¬ìš©ë˜ëŠ” ìµœì í™” ë°©ë²•ì— ì˜í•´ ì •ê·œí™”ê°€ ì ì ˆí•˜ê²Œ ì²˜ë¦¬ë©ë‹ˆë‹¤.


To enable stochastic optimization methods commonly used in
deep network training, we perform the normalization for
each mini-batch, and backpropagate the gradients through
the normalization parameter
> ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ êµìœ¡ì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í™•ë¥ ì  ìµœì í™” ë°©ë²•ì„ í™œì„±í™”í•˜ê¸°
> ìœ„í•´ ê° ë¯¸ë‹ˆ ë°°ì¹˜ì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ê³  ì •ê·œí™” ë§¤ê°œ ë³€ìˆ˜ë¥¼ í†µí•´
> ê·¸ë ˆì´ë””ì–¸íŠ¸ë¥¼ ì—­ ì „íŒŒí•œë‹¤.

Batch Normalization adds
only two extra parameters per activation, and in doing so
preserves the representation ability of the network. We
presented an algorithm for constructing, training, and performing
inference with batch-normalized networks. 
> ë°°ì¹˜ ì •ê·œí™”ëŠ” í™œì„±í™”ë‹¹ ë‘ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ë§Œ ì¶”ê°€í•˜ë©°, ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ì˜
> í‘œí˜„ ëŠ¥ë ¥ì„ ë³´ì¡´í•©ë‹ˆë‹¤. ë°°ì¹˜ ì •ê·œí™”ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ êµ¬ì„±, 
>  í›ˆë ¨ ë° ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí–ˆë‹¤.


The resulting networks can be trained with saturating 
nonlinearities,  are more tolerant to increased 
training rates, and  often do not require Dropout for regularization.
> ê²°ê³¼ ë„¤íŠ¸ì›Œí¬ëŠ” í¬í™” ë¹„ì„ í˜•ì„±ìœ¼ë¡œ í›ˆë ¨ë  ìˆ˜ ìˆê³ , í›ˆë ¨ ì†ë„ ì¦ê°€ì— 
> ë” ë‚´ì„±ì´ ìˆìœ¼ë©°, ì •ê·œí™”ë¥¼ ìœ„í•´ ë“œë¡­ì•„ì›ƒì„ ìš”êµ¬í•˜ì§€ ì•ŠëŠ”ë‹¤.

---

Merely adding Batch Normalization to a state-of-the art
image classification model yields a substantial speedup
in training. By further increasing the learning rates, removing
Dropout, and applying other modifications afforded
by Batch Normalization, we reach the previous
state of the art with only a small fraction of training steps
> ë°°ì¹˜ ì •ê·œí™”ë¥¼ ìµœì²¨ë‹¨ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì— ì¶”ê°€í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ í›ˆë ¨ ì†ë„ê°€ í¬ê²Œ í–¥ìƒëœë‹¤.
> í•™ìŠµë¥ ì„ ë”ìš± ë†’ì´ê³ , ë“œë¡­ì•„ì›ƒì„ ì œê±°í•˜ê³ , ë°°ì¹˜ ì •ê·œí™”ì— ì˜í•´ ì œê³µë˜ëŠ”
> ë‹¤ë¥¸ ìˆ˜ì • ì‚¬í•­ì„ ì ìš©í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” í›ˆë ¨ ë‹¨ê³„ì˜ ê·¹íˆ ì¼ë¶€ë§Œìœ¼ë¡œ ì´ì „ ìµœì‹  
> ê¸°ìˆ ì— ë„ë‹¬í•œë‹¤.

and then beat the state of the art in single-network image
classification. Furthermore, by combining multiple models
trained with Batch Normalization, we perform better than
the best known system on ImageNet, by a significant margin.
> ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬ ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤. ë˜í•œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ 
> í†µí•´ í›ˆë ¨ëœ ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ImageNetì—ì„œ ê°€ì¥ ì˜ ì•Œë ¤ì§„ ì‹œìŠ¤í…œë³´ë‹¤ 
> í›¨ì”¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤.

---
Interestingly, our method bears similarity to the standardization
layer , though the two methods stem from very different goals,
and perform  different tasks.
> í¥ë¯¸ë¡­ê²Œë„, ìš°ë¦¬ì˜ ë°©ë²•ì€ í‘œì¤€í™” ê³„ì¸µê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, 
> ë‘ ê°€ì§€ ë°©ë²•ì€ ë§¤ìš° ë‹¤ë¥¸ ëª©í‘œì—ì„œ ë¹„ë¡¯ë˜ê³  ë‹¤ë¥¸ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

The goal of Batch Normalization
is to achieve a stable distribution of activation values
throughout training, and in our experiments we apply it
before the nonlinearity since that is where matching the
first and second moments is more likely to result in a
stable distribution
> ë°°ì¹˜ ì •ê·œí™”ì˜ ëª©í‘œëŠ” í›ˆë ¨ ë‚´ë‚´ í™œì„±í™” ê°’ì˜ ì•ˆì •ì ì¸ ë¶„í¬ë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ë©°,
> ì‹¤í—˜ì—ì„œ ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ëª¨ë©˜íŠ¸ë¥¼ ì¼ì¹˜ì‹œí‚¤ëŠ” ê²ƒì´ ì•ˆì •ì ì¸ ë¶„í¬ë¥¼
> ì–»ì„ ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì— ë¹„ì„ í˜•ì„± ì „ì— ì ìš©í•œë‹¤.

On the contrary, apply the standardization layer to the output 
of the  nonlinearity, which results in sparser activations. 
In our large-scale image classification experiments, we have not
observed the nonlinearity inputs to be sparse, neither with
nor without Batch Normalization.
> ë°˜ëŒ€ë¡œ, í‘œì¤€í™” ë ˆì´ì–´ë¥¼ ë¹„ì„ í˜•ì„±ì˜ ì¶œë ¥ì— ì ìš©í•˜ë©´ í¬ì†Œ í™œì„±í™”ê°€ ë°œìƒí•œë‹¤.
> ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤í—˜ì—ì„œ ë°°ì¹˜ ì •ê·œí™”ê°€ ìˆë“  ì—†ë“  ë¹„ì„ í˜•ì„± ì…ë ¥ì´
> í¬ì†Œí•˜ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•˜ì§€ ëª»í–ˆë‹¤.

Other notable differentiating
characteristics of Batch Normalization include
the learned scale and shift that allow the BN transform
to represent identity (the standardization layer did not require
this since it was followed by the learned linear transform
that, conceptually, absorbs the necessary scale and
shift), handling of convolutional layers, deterministic inference
that does not depend on themini-batch, and batch-normalizing
each convolutional layer in the network.
> ë°°ì¹˜ ì •ê·œí™”ì˜ ë‹¤ë¥¸ ì£¼ëª©í•  ë§Œí•œ ì°¨ë³„í™” íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
> BN ë³€í™˜ì„ í†µí•´ ì •ì²´ì„±ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” í•™ìŠµëœ ê·œëª¨ì™€ ì´ë™
> (í‘œì¤€í™” ê³„ì¸µì€ ê°œë…ì ìœ¼ë¡œ í•„ìš”í•œ ê·œëª¨ì™€ ì´ë™ì„ í¡ìˆ˜í•˜ëŠ” í•™ìŠµëœ ì„ í˜• ë³€í™˜ì´ ë’¤ë”°ë¥´ê¸° 
> ë•Œë¬¸ì— í•„ìš”í•˜ì§€ ì•Šì•˜ë‹¤), ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µì˜ ì²˜ë¦¬, ë¯¸ë‹ˆ ë°°ì¹˜ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ”
> ê²°ì •ë¡ ì  ì¶”ë¡  ë° ë°°ì¹˜ ì—†ìŒë„¤íŠ¸ì›Œí¬ì˜ ê° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.

In this work, we have not explored the full range of
possibilities that Batch Normalization potentially enables.
Our future work includes applications of our method to
Recurrent Neural Networks (Pascanu et al., 2013), where
the internal covariate shift and the vanishing or exploding
gradients may be especially severe, and which would allow
us to more thoroughly test the hypothesis that normalization
improves gradient propagation (Sec. 3.3).
> ë³¸ ì—°êµ¬ì—ì„œëŠ” ë°°ì¹˜ ì •ê·œí™”ê°€ ì ì¬ì ìœ¼ë¡œ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ë²”ìœ„ì˜ ê°€ëŠ¥ì„±ì„
> íƒêµ¬í•˜ì§€ ì•Šì•˜ë‹¤. ìš°ë¦¬ì˜ í–¥í›„ ì—°êµ¬ëŠ” ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™ê³¼ ì†Œë©¸ ë˜ëŠ” í­ë°œ ê¸°ìš¸ê¸°ê°€
> íŠ¹íˆ ì‹¬í•  ìˆ˜ ìˆëŠ” ìˆœí™˜ ì‹ ê²½ë§ì— ëŒ€í•œ ìš°ë¦¬ì˜ ë°©ë²•ì˜ ì ìš©ì„ í¬í•¨í•˜ë©°,
> ì´ë¥¼ í†µí•´ ì •ê·œí™”ê°€ ê¸°ìš¸ê¸° ì „íŒŒë¥¼ ê°œì„ í•œë‹¤ëŠ” ê°€ì„¤ì„ ë”
> ì² ì €íˆ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‹¤(3.3í•­).